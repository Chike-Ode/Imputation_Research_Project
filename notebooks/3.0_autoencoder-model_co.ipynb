{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13d9593",
   "metadata": {},
   "source": [
    "# Imputation Research Project <img src=\"https://chroniclesofai.com/content/images/2021/05/file-20201210-18-elk4m.jpg\" alt=\"Alt text image not displaying\" width=\"360\" align=\"right\" />\n",
    "## Notebook 3.0: Autoencoder Model\n",
    "\n",
    "**Author:** Chike Odenigbo\n",
    "\n",
    "**Date:** November 25th, 2022\n",
    "\n",
    "**Notebook Structure:**\n",
    "\n",
    "* 1.0 Preprocessing\n",
    "\n",
    "* **1.1 Exploratory Data Analysis**\n",
    "\n",
    "* 1.2 Masking\n",
    "\n",
    "* 2.* Models\n",
    "\n",
    "\n",
    "Water Sugar lutein_zeaxanthin Alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d9e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from src.visualization.visualize import histogram, box_plot, bar_plot\n",
    "from pathlib import Path\n",
    "from notebook_config import ROOT_DIR  # setup.py file changed the root of the project so it is set in the config file\n",
    "\n",
    "ROOT_DIR = ROOT_DIR.as_posix()  # convert root path to windows readable path (i.e. change backslash to forward slash)\n",
    "from joblib import load\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "039e30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_nm = \"3.0-autoencoder\"\n",
    "fig_dir = f\"{ROOT_DIR}/reports/figures/\"\n",
    "model_dir = f\"{ROOT_DIR}/models/autoencoders/\"\n",
    "scaler_dir = f'{ROOT_DIR}/models/scalers/'\n",
    "output_prefix = notebook_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af97d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Ground Truth Included\n",
    "water_df = pd.read_csv(f'{ROOT_DIR}/data/processed/water.csv')\n",
    "sugars_df = pd.read_csv(f'{ROOT_DIR}/data/processed/sugars.csv')\n",
    "lutein_df = pd.read_csv(f'{ROOT_DIR}/data/processed/lutein.csv')\n",
    "\n",
    "# Scaled Data without Ground Truth to prevent Data Leakage, rows are included with NaN\n",
    "water_mcar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/water_mcar_scaled.csv', index_col = 0)\n",
    "water_mar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/water_mar_scaled.csv', index_col = 0)\n",
    "\n",
    "lutein_mar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/lutein_mar_scaled.csv', index_col = 0)\n",
    "lutein_mcar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/utein_mcar_scaled.csv', index_col = 0)\n",
    "\n",
    "sugars_mar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/sugars_mar_scaled.csv', index_col = 0)\n",
    "sugars_mcar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/sugars_mcar_scaled.csv', index_col = 0)\n",
    "\n",
    "water_mcar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/water_mcar_scaled.csv', index_col = 0)\n",
    "water_mar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/water_mar_scaled.csv', index_col = 0)\n",
    "\n",
    "# Scalers to return back to origin scale for model evaluation\n",
    "scaler_lutein_mar = load(f'{scaler_dir}/scaler_lutein_mar.joblib')\n",
    "scaler_lutein_mcar = load(f'{scaler_dir}/scaler_lutein_mcar.joblib')\n",
    "\n",
    "scaler_sugars_mar = load(f'{scaler_dir}/scaler_sugars_mar.joblib')\n",
    "scaler_sugars_mcar = load(f'{scaler_dir}/scaler_sugars_mcar.joblib')\n",
    "\n",
    "scaler_water_mcar = load(f'{scaler_dir}/scaler_water_mcar.joblib')\n",
    "scaler_water_mar = load(f'{scaler_dir}/scaler_water_mar.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed1b51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_ground_truth(scaler,raw_df,target_col,non_target_na_col):\n",
    "    raw_filtered_df = raw_df[raw_df[target_col].isnull()].drop(['name',target_col, non_target_na_col], axis = 'columns')\n",
    "    scaled_ground_truth_df = pd.DataFrame(scaler.transform(raw_filtered_df),columns = raw_filtered_df.columns)\n",
    "    return scaled_ground_truth_df\n",
    "\n",
    "def train_test_split_scaled(scaled_df,drop_cols = ['name','dataset_type']):\n",
    "    train_df = scaled_df[scaled_df.dataset_type == 'training'].drop(drop_cols, axis = 'columns')\n",
    "    val_df = scaled_df[scaled_df.dataset_type == 'validation'].drop(drop_cols, axis = 'columns')\n",
    "    return train_df, val_df\n",
    "\n",
    "def autoencoder(data, input_shape = 72, latent_shape = 40, patience = 2, lr = 0.0001):\n",
    "    enc_input = keras.Input(shape = (input_shape,),name = 'input_data')\n",
    "    latent_layer = keras.layers.Dense(latent_shape, activation = 'relu')(enc_input)\n",
    "    encoder = keras.Model(enc_input,latent_layer, name = \"encoder\")\n",
    "    dec_output = keras.layers.Dense(input_shape)(latent_layer)\n",
    "    opt = keras.optimizers.Adam(lr = lr)\n",
    "    auto_encoder = keras.Model(enc_input,dec_output, name=\"auto_encoder\")\n",
    "    print(auto_encoder.summary())\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    auto_encoder.compile(opt, loss = \"mse\")\n",
    "    history = auto_encoder.fit(data, data, epochs = 1000, validation_split = 0.1, callbacks=[callback])\n",
    "    return auto_encoder, history\n",
    "\n",
    "def save_autoencoder(model,model_name,path = model_dir):\n",
    "    model.save(f'{path}/{model_name}.h5')\n",
    "    with open(f'{path}/{model_name}_history.pkl', 'wb') as file_pi:\n",
    "        pickle.dump(model.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8cfc20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_mcar_train_df, water_mcar_val_df = train_test_split_scaled(water_mcar_scaled_df,drop_cols = ['name','dataset_type'])\n",
    "water_mar_train_df, water_mar_val_df = train_test_split_scaled(water_mar_scaled_df,drop_cols = ['name','dataset_type'])\n",
    "\n",
    "sugars_mcar_train_df, sugars_mcar_val_df = train_test_split_scaled(sugars_mcar_scaled_df,drop_cols = ['name','dataset_type'])\n",
    "sugars_mar_train_df, sugars_mar_val_df = train_test_split_scaled(sugars_mar_scaled_df,drop_cols = ['name','dataset_type'])\n",
    "\n",
    "lutein_mcar_train_df, lutein_mcar_val_df = train_test_split_scaled(lutein_mcar_scaled_df,drop_cols = ['name','dataset_type'])\n",
    "lutein_mar_train_df, lutein_mar_val_df = train_test_split_scaled(lutein_mar_scaled_df,drop_cols = ['name','dataset_type'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d34bc",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2423227",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_data (InputLayer)     [(None, 72)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 40)                2920      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 72)                2952      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,872\n",
      "Trainable params: 5,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/1000\n",
      "140/140 [==============================] - 2s 11ms/step - loss: 0.9123 - val_loss: 0.1546\n",
      "Epoch 2/1000\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.5648 - val_loss: 0.1112\n",
      "Epoch 3/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.4339 - val_loss: 0.0971\n",
      "Epoch 4/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.3500 - val_loss: 0.0839\n",
      "Epoch 5/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2925 - val_loss: 0.0720\n",
      "Epoch 6/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2490 - val_loss: 0.0626\n",
      "Epoch 7/1000\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.2154 - val_loss: 0.0570\n",
      "Epoch 8/1000\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1877 - val_loss: 0.0510\n",
      "Epoch 9/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.1660 - val_loss: 0.0484\n",
      "Epoch 10/1000\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.1467 - val_loss: 0.0452\n",
      "Epoch 11/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1327 - val_loss: 0.0406\n",
      "Epoch 12/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1184 - val_loss: 0.0389\n",
      "Epoch 13/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1077 - val_loss: 0.0355\n",
      "Epoch 14/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.0977 - val_loss: 0.0334\n",
      "Epoch 15/1000\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.0897 - val_loss: 0.0320\n",
      "Epoch 16/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.0820 - val_loss: 0.0306\n",
      "Epoch 17/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.0755 - val_loss: 0.0292\n",
      "Epoch 18/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.0699 - val_loss: 0.0294\n",
      "Epoch 19/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.0663 - val_loss: 0.0273\n",
      "Epoch 20/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.0626 - val_loss: 0.0265\n",
      "Epoch 21/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.0571 - val_loss: 0.0260\n",
      "Epoch 22/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.0545 - val_loss: 0.0238\n",
      "Epoch 23/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.0510 - val_loss: 0.0226\n",
      "Epoch 24/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.0486 - val_loss: 0.0223\n",
      "Epoch 25/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.0466 - val_loss: 0.0213\n",
      "Epoch 26/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.0447 - val_loss: 0.0196\n",
      "Epoch 27/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.0427 - val_loss: 0.0196\n",
      "Epoch 28/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.0413 - val_loss: 0.0176\n",
      "Epoch 29/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.0410 - val_loss: 0.0172\n",
      "Epoch 30/1000\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.0394 - val_loss: 0.0158\n",
      "Epoch 31/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.0406 - val_loss: 0.0152\n",
      "Epoch 32/1000\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.0372 - val_loss: 0.0152\n",
      "Epoch 33/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.0367 - val_loss: 0.0136\n",
      "Epoch 34/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.0359 - val_loss: 0.0138\n",
      "Epoch 35/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.0360 - val_loss: 0.0133\n",
      "Epoch 36/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.0360 - val_loss: 0.0131\n",
      "Epoch 37/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.0354 - val_loss: 0.0128\n",
      "Epoch 38/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.0346 - val_loss: 0.0132\n",
      "Epoch 39/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.0348 - val_loss: 0.0127\n",
      "Epoch 40/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.0349 - val_loss: 0.0130\n",
      "Epoch 41/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.0355 - val_loss: 0.0135\n"
     ]
    }
   ],
   "source": [
    "sugars_mcar_model, sugars_mcar_history = autoencoder(sugars_mcar_train_df, input_shape = 72, latent_shape = 40, patience = 2, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8e47e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_data (InputLayer)     [(None, 72)]              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 40)                2920      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 72)                2952      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,872\n",
      "Trainable params: 5,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "183/183 [==============================] - 3s 8ms/step - loss: 0.5843 - val_loss: 0.1248\n",
      "Epoch 2/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.3230 - val_loss: 0.0870\n",
      "Epoch 3/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.2404 - val_loss: 0.0685\n",
      "Epoch 4/1000\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.1897 - val_loss: 0.0579\n",
      "Epoch 5/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.1562 - val_loss: 0.0480\n",
      "Epoch 6/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.1318 - val_loss: 0.0441\n",
      "Epoch 7/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.1138 - val_loss: 0.0416\n",
      "Epoch 8/1000\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0991 - val_loss: 0.0382\n",
      "Epoch 9/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0870 - val_loss: 0.0378\n",
      "Epoch 10/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0765 - val_loss: 0.0348\n",
      "Epoch 11/1000\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0677 - val_loss: 0.0330\n",
      "Epoch 12/1000\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0606 - val_loss: 0.0319\n",
      "Epoch 13/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0546 - val_loss: 0.0299\n",
      "Epoch 14/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0498 - val_loss: 0.0279\n",
      "Epoch 15/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0453 - val_loss: 0.0255\n",
      "Epoch 16/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0416 - val_loss: 0.0224\n",
      "Epoch 17/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0385 - val_loss: 0.0211\n",
      "Epoch 18/1000\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0359 - val_loss: 0.0189\n",
      "Epoch 19/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0334 - val_loss: 0.0188\n",
      "Epoch 20/1000\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0316 - val_loss: 0.0169\n",
      "Epoch 21/1000\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0301 - val_loss: 0.0157\n",
      "Epoch 22/1000\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0288 - val_loss: 0.0150\n",
      "Epoch 23/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0277 - val_loss: 0.0147\n",
      "Epoch 24/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0270 - val_loss: 0.0143\n",
      "Epoch 25/1000\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0145\n",
      "Epoch 26/1000\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0259 - val_loss: 0.0127\n",
      "Epoch 27/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0252 - val_loss: 0.0129\n",
      "Epoch 28/1000\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0249 - val_loss: 0.0123\n",
      "Epoch 29/1000\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0247 - val_loss: 0.0122\n",
      "Epoch 30/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0247 - val_loss: 0.0124\n",
      "Epoch 31/1000\n",
      "183/183 [==============================] - 2s 9ms/step - loss: 0.0243 - val_loss: 0.0132\n"
     ]
    }
   ],
   "source": [
    "sugars_mar_model, sugars_mar_history = autoencoder(sugars_mar_train_df, input_shape = 72, latent_shape = 40, patience = 2, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ddd88c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_data (InputLayer)     [(None, 72)]              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 40)                2920      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 72)                2952      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,872\n",
      "Trainable params: 5,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "183/183 [==============================] - 2s 7ms/step - loss: 0.5912 - val_loss: 0.1174\n",
      "Epoch 2/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.3338 - val_loss: 0.0848\n",
      "Epoch 3/1000\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.2461 - val_loss: 0.0684\n",
      "Epoch 4/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.1928 - val_loss: 0.0507\n",
      "Epoch 5/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.1585 - val_loss: 0.0428\n",
      "Epoch 6/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.1338 - val_loss: 0.0384\n",
      "Epoch 7/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.1154 - val_loss: 0.0351\n",
      "Epoch 8/1000\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.1011 - val_loss: 0.0321\n",
      "Epoch 9/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0889 - val_loss: 0.0298\n",
      "Epoch 10/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0788 - val_loss: 0.0280\n",
      "Epoch 11/1000\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0699 - val_loss: 0.0264\n",
      "Epoch 12/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0625 - val_loss: 0.0255\n",
      "Epoch 13/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0559 - val_loss: 0.0241\n",
      "Epoch 14/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0503 - val_loss: 0.0230\n",
      "Epoch 15/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0461 - val_loss: 0.0220\n",
      "Epoch 16/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0424 - val_loss: 0.0213\n",
      "Epoch 17/1000\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0395 - val_loss: 0.0190\n",
      "Epoch 18/1000\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0371 - val_loss: 0.0187\n",
      "Epoch 19/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0350 - val_loss: 0.0184\n",
      "Epoch 20/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0334 - val_loss: 0.0168\n",
      "Epoch 21/1000\n",
      "183/183 [==============================] - 2s 10ms/step - loss: 0.0320 - val_loss: 0.0168\n",
      "Epoch 22/1000\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 0.0307 - val_loss: 0.0165\n",
      "Epoch 23/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0301 - val_loss: 0.0155\n",
      "Epoch 24/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0291 - val_loss: 0.0145\n",
      "Epoch 25/1000\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0284 - val_loss: 0.0145\n",
      "Epoch 26/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0280 - val_loss: 0.0141\n",
      "Epoch 27/1000\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0273 - val_loss: 0.0146\n",
      "Epoch 28/1000\n",
      "183/183 [==============================] - 1s 6ms/step - loss: 0.0270 - val_loss: 0.0136\n",
      "Epoch 29/1000\n",
      "183/183 [==============================] - 2s 8ms/step - loss: 0.0269 - val_loss: 0.0133\n",
      "Epoch 30/1000\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0264 - val_loss: 0.0129\n",
      "Epoch 31/1000\n",
      "183/183 [==============================] - 1s 8ms/step - loss: 0.0258 - val_loss: 0.0140\n",
      "Epoch 32/1000\n",
      "183/183 [==============================] - 1s 5ms/step - loss: 0.0260 - val_loss: 0.0129\n"
     ]
    }
   ],
   "source": [
    "water_mar_model, water_mar_history = autoencoder(water_mar_train_df, input_shape = 72, latent_shape = 40, patience = 2, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba196db1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_data (InputLayer)     [(None, 72)]              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 40)                2920      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 72)                2952      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,872\n",
      "Trainable params: 5,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "161/161 [==============================] - 2s 8ms/step - loss: 0.9285 - val_loss: 0.2194\n",
      "Epoch 2/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.5629 - val_loss: 0.1610\n",
      "Epoch 3/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.4232 - val_loss: 0.1427\n",
      "Epoch 4/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.3386 - val_loss: 0.1241\n",
      "Epoch 5/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.2821 - val_loss: 0.1090\n",
      "Epoch 6/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.2414 - val_loss: 0.0992\n",
      "Epoch 7/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.2072 - val_loss: 0.0904\n",
      "Epoch 8/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.1804 - val_loss: 0.0844\n",
      "Epoch 9/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.1591 - val_loss: 0.0786\n",
      "Epoch 10/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.1416 - val_loss: 0.0756\n",
      "Epoch 11/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.1268 - val_loss: 0.0735\n",
      "Epoch 12/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.1142 - val_loss: 0.0695\n",
      "Epoch 13/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.1028 - val_loss: 0.0674\n",
      "Epoch 14/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0940 - val_loss: 0.0620\n",
      "Epoch 15/1000\n",
      "161/161 [==============================] - 1s 9ms/step - loss: 0.0858 - val_loss: 0.0616\n",
      "Epoch 16/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0786 - val_loss: 0.0591\n",
      "Epoch 17/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0718 - val_loss: 0.0561\n",
      "Epoch 18/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0676 - val_loss: 0.0550\n",
      "Epoch 19/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0619 - val_loss: 0.0516\n",
      "Epoch 20/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0585 - val_loss: 0.0512\n",
      "Epoch 21/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0553 - val_loss: 0.0499\n",
      "Epoch 22/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0539 - val_loss: 0.0486\n",
      "Epoch 23/1000\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.0505 - val_loss: 0.0451\n",
      "Epoch 24/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0483 - val_loss: 0.0429\n",
      "Epoch 25/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0466 - val_loss: 0.0423\n",
      "Epoch 26/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0449 - val_loss: 0.0416\n",
      "Epoch 27/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0438 - val_loss: 0.0414\n",
      "Epoch 28/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0425 - val_loss: 0.0409\n",
      "Epoch 29/1000\n",
      "161/161 [==============================] - 1s 8ms/step - loss: 0.0426 - val_loss: 0.0405\n",
      "Epoch 30/1000\n",
      "161/161 [==============================] - 1s 9ms/step - loss: 0.0424 - val_loss: 0.0403\n",
      "Epoch 31/1000\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.0403 - val_loss: 0.0405\n",
      "Epoch 32/1000\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.0411 - val_loss: 0.0403\n"
     ]
    }
   ],
   "source": [
    "water_mcar_model, water_mcar_history = autoencoder(water_mcar_train_df, input_shape = 72, latent_shape = 40, patience = 2, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9a7e5f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_data (InputLayer)     [(None, 72)]              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 40)                2920      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 72)                2952      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,872\n",
      "Trainable params: 5,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "162/162 [==============================] - 2s 8ms/step - loss: 0.6565 - val_loss: 0.1269\n",
      "Epoch 2/1000\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 0.3778 - val_loss: 0.0953\n",
      "Epoch 3/1000\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.2831 - val_loss: 0.0797\n",
      "Epoch 4/1000\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 0.2241 - val_loss: 0.0678\n",
      "Epoch 5/1000\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.1840 - val_loss: 0.0592\n",
      "Epoch 6/1000\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 0.1542 - val_loss: 0.0522\n",
      "Epoch 7/1000\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 0.1318 - val_loss: 0.0472\n",
      "Epoch 8/1000\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 0.1147 - val_loss: 0.0436\n",
      "Epoch 9/1000\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 0.1004 - val_loss: 0.0397\n",
      "Epoch 10/1000\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 0.0887 - val_loss: 0.0366\n",
      "Epoch 11/1000\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 0.0789 - val_loss: 0.0345\n",
      "Epoch 12/1000\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 0.0710 - val_loss: 0.0320\n",
      "Epoch 13/1000\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0642 - val_loss: 0.0297\n",
      "Epoch 14/1000\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 0.0583 - val_loss: 0.0284\n",
      "Epoch 15/1000\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0538 - val_loss: 0.0271\n",
      "Epoch 16/1000\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0496 - val_loss: 0.0262\n",
      "Epoch 17/1000\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 0.0463 - val_loss: 0.0253\n",
      "Epoch 18/1000\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0432 - val_loss: 0.0237\n",
      "Epoch 19/1000\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0409 - val_loss: 0.0232\n",
      "Epoch 20/1000\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 0.0386 - val_loss: 0.0213\n",
      "Epoch 21/1000\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 0.0364 - val_loss: 0.0197\n",
      "Epoch 22/1000\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 0.0343 - val_loss: 0.0192\n",
      "Epoch 23/1000\n",
      "162/162 [==============================] - 1s 5ms/step - loss: 0.0324 - val_loss: 0.0161\n",
      "Epoch 24/1000\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 0.0309 - val_loss: 0.0149\n",
      "Epoch 25/1000\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 0.0294 - val_loss: 0.0132\n",
      "Epoch 26/1000\n",
      "162/162 [==============================] - 1s 6ms/step - loss: 0.0286 - val_loss: 0.0131\n",
      "Epoch 27/1000\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0277 - val_loss: 0.0129\n",
      "Epoch 28/1000\n",
      "162/162 [==============================] - 2s 9ms/step - loss: 0.0271 - val_loss: 0.0123\n",
      "Epoch 29/1000\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 0.0268 - val_loss: 0.0111\n",
      "Epoch 30/1000\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 0.0262 - val_loss: 0.0113\n",
      "Epoch 31/1000\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0260 - val_loss: 0.0113\n"
     ]
    }
   ],
   "source": [
    "lutein_mar_model, lutein_mar_history = autoencoder(lutein_mar_train_df, input_shape = 72, latent_shape = 40, patience = 2, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "806805d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_data (InputLayer)     [(None, 72)]              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 40)                2920      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 72)                2952      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,872\n",
      "Trainable params: 5,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "140/140 [==============================] - 2s 9ms/step - loss: 0.9269 - val_loss: 0.1555\n",
      "Epoch 2/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.5618 - val_loss: 0.1064\n",
      "Epoch 3/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.4354 - val_loss: 0.0839\n",
      "Epoch 4/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.3523 - val_loss: 0.0735\n",
      "Epoch 5/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.2933 - val_loss: 0.0614\n",
      "Epoch 6/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.2518 - val_loss: 0.0592\n",
      "Epoch 7/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2205 - val_loss: 0.0539\n",
      "Epoch 8/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1951 - val_loss: 0.0504\n",
      "Epoch 9/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1736 - val_loss: 0.0547\n",
      "Epoch 10/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1566 - val_loss: 0.0437\n",
      "Epoch 11/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.1407 - val_loss: 0.0411\n",
      "Epoch 12/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1267 - val_loss: 0.0407\n",
      "Epoch 13/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1148 - val_loss: 0.0371\n",
      "Epoch 14/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.1050 - val_loss: 0.0391\n",
      "Epoch 15/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.0953 - val_loss: 0.0364\n",
      "Epoch 16/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.0880 - val_loss: 0.0328\n",
      "Epoch 17/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.0808 - val_loss: 0.0312\n",
      "Epoch 18/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.0747 - val_loss: 0.0310\n",
      "Epoch 19/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.0698 - val_loss: 0.0284\n",
      "Epoch 20/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.0651 - val_loss: 0.0279\n",
      "Epoch 21/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.0613 - val_loss: 0.0258\n",
      "Epoch 22/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.0576 - val_loss: 0.0237\n",
      "Epoch 23/1000\n",
      "140/140 [==============================] - 1s 8ms/step - loss: 0.0544 - val_loss: 0.0229\n",
      "Epoch 24/1000\n",
      "140/140 [==============================] - 1s 6ms/step - loss: 0.0518 - val_loss: 0.0211\n",
      "Epoch 25/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.0495 - val_loss: 0.0194\n",
      "Epoch 26/1000\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.0474 - val_loss: 0.0184\n",
      "Epoch 27/1000\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 0.0458 - val_loss: 0.0175\n",
      "Epoch 28/1000\n",
      "140/140 [==============================] - 1s 9ms/step - loss: 0.0434 - val_loss: 0.0183\n",
      "Epoch 29/1000\n",
      "140/140 [==============================] - 1s 10ms/step - loss: 0.0423 - val_loss: 0.0224\n"
     ]
    }
   ],
   "source": [
    "lutein_mcar_model, lutein_mcar_history = autoencoder(lutein_mcar_train_df, input_shape = 72, latent_shape = 40, patience = 2, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2f34278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 17:04:08         1872\n",
      "metadata.json                                  2022-11-28 17:04:08           64\n",
      "variables.h5                                   2022-11-28 17:04:08        36504\n"
     ]
    }
   ],
   "source": [
    "save_autoencoder(model = sugars_mcar_model, model_name = 'sugars_mcar_model',path = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93a3d8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 17:04:08         1878\n",
      "metadata.json                                  2022-11-28 17:04:08           64\n",
      "variables.h5                                   2022-11-28 17:04:08        36504\n"
     ]
    }
   ],
   "source": [
    "save_autoencoder(model = sugars_mar_model, model_name = 'sugars_mar_model',path = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3bc82ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 17:04:08         1878\n",
      "metadata.json                                  2022-11-28 17:04:08           64\n",
      "variables.h5                                   2022-11-28 17:04:08        36504\n"
     ]
    }
   ],
   "source": [
    "save_autoencoder(model = water_mar_model, model_name = 'water_mar_model',path = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4ff3ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 17:04:08         1878\n",
      "metadata.json                                  2022-11-28 17:04:08           64\n",
      "variables.h5                                   2022-11-28 17:04:08        36504\n"
     ]
    }
   ],
   "source": [
    "save_autoencoder(model = water_mcar_model, model_name = 'water_mcar_model',path = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0496c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 17:04:09         1878\n",
      "metadata.json                                  2022-11-28 17:04:09           64\n",
      "variables.h5                                   2022-11-28 17:04:09        36504\n"
     ]
    }
   ],
   "source": [
    "save_autoencoder(model = lutein_mar_model, model_name = 'lutein_mar_model',path = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4611f0bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 17:04:09         1884\n",
      "metadata.json                                  2022-11-28 17:04:09           64\n",
      "variables.h5                                   2022-11-28 17:04:09        36504\n"
     ]
    }
   ],
   "source": [
    "save_autoencoder(model = lutein_mcar_model, model_name = 'lutein_mcar_model',path = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8078e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_water_mcar = water_mcar_train_df.water_mcar.max()\n",
    "min_water_mcar = water_mcar_train_df.water_mcar.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba50750a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'water_mcar_test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mwater_mcar_test_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwater_mcar\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\u001b[38;5;66;03m#np.random.uniform(min_water_mcar, max_water_mcar, size=len(water_mcar_test_df))\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'water_mcar_test_df' is not defined"
     ]
    }
   ],
   "source": [
    "water_mcar_test_df['water_mcar'] = 0#np.random.uniform(min_water_mcar, max_water_mcar, size=len(water_mcar_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730bb124",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_mcar_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e72f92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "water_mcar_pred = pd.DataFrame(auto_encoder.predict(water_mcar_test_df), columns = water_mcar_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9028b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "water_mcar_true = pd.DataFrame(scaler_water_mcar.transform(water_df[water_df.water_mcar.isnull()].drop(['name','water_mcar', 'water_mar'], axis = 'columns')),columns = water_df.drop(['name','water_mcar', 'water_mar'], axis = 'columns').columns)# a = np.array(x) # your x\n",
    "# b = np.array(y) # your y\n",
    "# mses = ((a-b)**2).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca26b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "pd.DataFrame({'columns':water_mcar_true.columns,\n",
    "              'mse':list(mean_squared_error(water_mcar_true, water_mcar_pred, multioutput='raw_values'))}).nlargest(10,'mse')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imputation_research",
   "language": "python",
   "name": "imputation_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
