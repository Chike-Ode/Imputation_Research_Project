{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13d9593",
   "metadata": {},
   "source": [
    "# Imputation Research Project <img src=\"https://chroniclesofai.com/content/images/2021/05/file-20201210-18-elk4m.jpg\" alt=\"Alt text image not displaying\" width=\"360\" align=\"right\" />\n",
    "## Notebook 3.0: Autoencoder Model\n",
    "\n",
    "**Author:** Chike Odenigbo\n",
    "\n",
    "**Date:** November 25th, 2022\n",
    "\n",
    "**Notebook Structure:**\n",
    "\n",
    "* 1.0 Preprocessing\n",
    "\n",
    "* **1.1 Exploratory Data Analysis**\n",
    "\n",
    "* 1.2 Masking\n",
    "\n",
    "* 2.* Models\n",
    "\n",
    "\n",
    "Water Sugar lutein_zeaxanthin Alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d9e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from src.visualization.visualize import histogram, box_plot, bar_plot\n",
    "from pathlib import Path\n",
    "from notebook_config import ROOT_DIR  # setup.py file changed the root of the project so it is set in the config file\n",
    "\n",
    "ROOT_DIR = ROOT_DIR.as_posix()  # convert root path to windows readable path (i.e. change backslash to forward slash)\n",
    "from joblib import load\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "039e30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_nm = \"3.0-autoencoder\"\n",
    "fig_dir = f\"{ROOT_DIR}/reports/figures/\"\n",
    "model_dir = f\"{ROOT_DIR}/models/autoencoders/\"\n",
    "scaler_dir = f'{ROOT_DIR}/models/scalers/'\n",
    "output_prefix = notebook_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3af97d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Ground Truth Included\n",
    "water_df = pd.read_csv(f'{ROOT_DIR}/data/processed/water.csv')\n",
    "sugars_df = pd.read_csv(f'{ROOT_DIR}/data/processed/sugars.csv')\n",
    "lutein_df = pd.read_csv(f'{ROOT_DIR}/data/processed/lutein.csv')\n",
    "\n",
    "# Scaled Data without Ground Truth to prevent Data Leakage, rows are included with NaN\n",
    "water_mcar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/water_mcar_scaled.csv', index_col = 0)\n",
    "water_mar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/water_mar_scaled.csv', index_col = 0)\n",
    "\n",
    "lutein_mar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/lutein_mar_scaled.csv', index_col = 0)\n",
    "lutein_mcar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/utein_mcar_scaled.csv', index_col = 0)\n",
    "\n",
    "sugars_mar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/sugars_mar_scaled.csv', index_col = 0)\n",
    "sugars_mcar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/sugars_mcar_scaled.csv', index_col = 0)\n",
    "\n",
    "water_mcar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/water_mcar_scaled.csv', index_col = 0)\n",
    "water_mar_scaled_df = pd.read_csv(f'{ROOT_DIR}/data/processed/water_mar_scaled.csv', index_col = 0)\n",
    "\n",
    "# Scalers to return back to origin scale for model evaluation\n",
    "scaler_lutein_mar = load(f'{scaler_dir}/scaler_lutein_mar.joblib')\n",
    "scaler_lutein_mcar = load(f'{scaler_dir}/scaler_lutein_mcar.joblib')\n",
    "\n",
    "scaler_sugars_mar = load(f'{scaler_dir}/scaler_sugars_mar.joblib')\n",
    "scaler_sugars_mcar = load(f'{scaler_dir}/scaler_sugars_mcar.joblib')\n",
    "\n",
    "scaler_water_mcar = load(f'{scaler_dir}/scaler_water_mcar.joblib')\n",
    "scaler_water_mar = load(f'{scaler_dir}/scaler_water_mar.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee76883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_ground_truth(scaler,raw_df,target_col,non_target_na_col):\n",
    "    raw_filtered_df = raw_df[raw_df[target_col].isnull()].drop(['name',target_col, non_target_na_col], axis = 'columns')\n",
    "    scaled_ground_truth_df = pd.DataFrame(scaler.transform(raw_filtered_df),columns = raw_filtered_df.columns)\n",
    "    return scaled_ground_truth_df\n",
    "\n",
    "def train_test_split_scaled(scaled_df,drop_cols = ['name','dataset_type','serving_size']):\n",
    "    train_df = scaled_df[scaled_df.dataset_type == 'training'].drop(drop_cols, axis = 'columns')\n",
    "    val_df = scaled_df[scaled_df.dataset_type == 'validation'].drop(drop_cols, axis = 'columns')\n",
    "    return train_df, val_df\n",
    "\n",
    "def autoencoder(data, input_shape = 72, latent_shape = 40, patience = 2, lr = 0.0001):\n",
    "    enc_input = keras.Input(shape = (input_shape,),name = 'input_data')\n",
    "    latent_layer = keras.layers.Dense(latent_shape, activation = 'relu')(enc_input)\n",
    "    encoder = keras.Model(enc_input,latent_layer, name = \"encoder\")\n",
    "    dec_output = keras.layers.Dense(input_shape)(latent_layer)\n",
    "    opt = keras.optimizers.Adam(lr = lr)\n",
    "    auto_encoder = keras.Model(enc_input,dec_output, name=\"auto_encoder\")\n",
    "    print(auto_encoder.summary())\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    auto_encoder.compile(opt, loss = \"mse\")\n",
    "    history = auto_encoder.fit(data, data, epochs = 1000, validation_split = 0.1, callbacks=[callback])\n",
    "    return auto_encoder, history\n",
    "\n",
    "def save_autoencoder(model,model_name,path = model_dir):\n",
    "    model.save(f'{path}/{model_name}.h5')\n",
    "    with open(f'{path}/{model_name}_history.pkl', 'wb') as file_pi:\n",
    "        pickle.dump(model.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf3c9998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- water\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- water_mcar\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- water\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- water_mar\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- sugars\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- sugars_mcar\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- sugars\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- sugars_mar\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- lutein_zeaxanthin\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- lutein_zeaxanthin_mcar\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- lutein_zeaxanthin\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- lutein_zeaxanthin_mar\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "water_mcar_ground_truth_scaled_df = scale_ground_truth(scaler = scaler_water_mcar,raw_df = water_df,target_col = 'water_mcar',non_target_na_col = 'water_mar')\n",
    "water_mar_ground_truth_scaled_df = scale_ground_truth(scaler = scaler_water_mar,raw_df = water_df,target_col = 'water_mar',non_target_na_col = 'water_mcar')\n",
    "\n",
    "sugars_mcar_ground_truth_scaled_df = scale_ground_truth(scaler = scaler_sugars_mcar,raw_df = sugars_df,target_col = 'sugars_mcar',non_target_na_col = 'sugars_mar')\n",
    "sugars_mar_ground_truth_scaled_df = scale_ground_truth(scaler = scaler_sugars_mar,raw_df = sugars_df,target_col = 'sugars_mar',non_target_na_col = 'sugars_mcar')\n",
    "\n",
    "lutein_mcar_ground_truth_scaled_df = scale_ground_truth(scaler = scaler_lutein_mcar,raw_df = lutein_df,target_col = 'lutein_zeaxanthin_mcar',non_target_na_col = 'lutein_zeaxanthin_mar')\n",
    "lutein_mar_ground_truth_scaled_df = scale_ground_truth(scaler = scaler_lutein_mar,raw_df = lutein_df,target_col = 'lutein_zeaxanthin_mar',non_target_na_col = 'lutein_zeaxanthin_mcar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef91ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_mcar_train_df, water_mcar_val_df = train_test_split_scaled(water_mcar_scaled_df,drop_cols = ['name','dataset_type','serving_size'])\n",
    "water_mar_train_df, water_mar_val_df = train_test_split_scaled(water_mar_scaled_df,drop_cols = ['name','dataset_type','serving_size'])\n",
    "\n",
    "sugars_mcar_train_df, sugars_mcar_val_df = train_test_split_scaled(sugars_mcar_scaled_df,drop_cols = ['name','dataset_type','serving_size'])\n",
    "sugars_mar_train_df, sugars_mar_val_df = train_test_split_scaled(sugars_mar_scaled_df,drop_cols = ['name','dataset_type','serving_size'])\n",
    "\n",
    "lutein_mcar_train_df, lutein_mcar_val_df = train_test_split_scaled(lutein_mcar_scaled_df,drop_cols = ['name','dataset_type','serving_size'])\n",
    "lutein_mar_train_df, lutein_mar_val_df = train_test_split_scaled(lutein_mar_scaled_df,drop_cols = ['name','dataset_type','serving_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c6ed47d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_data (InputLayer)     [(None, 72)]              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 40)                2920      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 72)                2952      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,872\n",
      "Trainable params: 5,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 1s 4ms/step - loss: 0.8909 - val_loss: 0.1615\n",
      "Epoch 2/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5482 - val_loss: 0.1129\n",
      "Epoch 3/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.4145 - val_loss: 0.0921\n",
      "Epoch 4/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.3333 - val_loss: 0.0795\n",
      "Epoch 5/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.2784 - val_loss: 0.0700\n",
      "Epoch 6/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.2375 - val_loss: 0.0632\n",
      "Epoch 7/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.2067 - val_loss: 0.0542\n",
      "Epoch 8/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1816 - val_loss: 0.0503\n",
      "Epoch 9/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.1616 - val_loss: 0.0473\n",
      "Epoch 10/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.1437 - val_loss: 0.0430\n",
      "Epoch 11/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1291 - val_loss: 0.0413\n",
      "Epoch 12/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.1171 - val_loss: 0.0392\n",
      "Epoch 13/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.1056 - val_loss: 0.0389\n",
      "Epoch 14/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0957 - val_loss: 0.0361\n",
      "Epoch 15/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0868 - val_loss: 0.0344\n",
      "Epoch 16/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0791 - val_loss: 0.0325\n",
      "Epoch 17/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0308\n",
      "Epoch 18/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0675 - val_loss: 0.0300\n",
      "Epoch 19/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0628 - val_loss: 0.0288\n",
      "Epoch 20/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0594 - val_loss: 0.0264\n",
      "Epoch 21/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0555 - val_loss: 0.0242\n",
      "Epoch 22/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0535 - val_loss: 0.0242\n",
      "Epoch 23/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0508 - val_loss: 0.0228\n",
      "Epoch 24/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0494 - val_loss: 0.0218\n",
      "Epoch 25/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0471 - val_loss: 0.0190\n",
      "Epoch 26/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0451 - val_loss: 0.0181\n",
      "Epoch 27/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0440 - val_loss: 0.0175\n",
      "Epoch 28/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0423 - val_loss: 0.0171\n",
      "Epoch 29/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0415 - val_loss: 0.0160\n",
      "Epoch 30/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0405 - val_loss: 0.0160\n",
      "Epoch 31/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0399 - val_loss: 0.0177\n",
      "Epoch 32/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.0154\n",
      "Epoch 33/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.0142\n",
      "Epoch 34/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0393 - val_loss: 0.0137\n",
      "Epoch 35/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0376 - val_loss: 0.0145\n",
      "Epoch 36/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0374 - val_loss: 0.0145\n"
     ]
    }
   ],
   "source": [
    "sugars_mcar_model, sugars_mcar_history = autoencoder(sugars_mcar_train_df, input_shape = 72, latent_shape = 40, patience = 2, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4091ef2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_data (InputLayer)     [(None, 72)]              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 40)                2920      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 72)                2952      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,872\n",
      "Trainable params: 5,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/1000\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.5773 - val_loss: 0.1097\n",
      "Epoch 2/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.3271 - val_loss: 0.0787\n",
      "Epoch 3/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.2425 - val_loss: 0.0609\n",
      "Epoch 4/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.1885 - val_loss: 0.0511\n",
      "Epoch 5/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.1530 - val_loss: 0.0443\n",
      "Epoch 6/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.1278 - val_loss: 0.0423\n",
      "Epoch 7/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.1090 - val_loss: 0.0391\n",
      "Epoch 8/1000\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0947 - val_loss: 0.0352\n",
      "Epoch 9/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0831 - val_loss: 0.0338\n",
      "Epoch 10/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0736 - val_loss: 0.0315\n",
      "Epoch 11/1000\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0654 - val_loss: 0.0295\n",
      "Epoch 12/1000\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0587 - val_loss: 0.0282\n",
      "Epoch 13/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0531 - val_loss: 0.0273\n",
      "Epoch 14/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0488 - val_loss: 0.0257\n",
      "Epoch 15/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0450 - val_loss: 0.0246\n",
      "Epoch 16/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.0233\n",
      "Epoch 17/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0388 - val_loss: 0.0210\n",
      "Epoch 18/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0366 - val_loss: 0.0205\n",
      "Epoch 19/1000\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0189\n",
      "Epoch 20/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0330 - val_loss: 0.0164\n",
      "Epoch 21/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0314 - val_loss: 0.0152\n",
      "Epoch 22/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0303 - val_loss: 0.0147\n",
      "Epoch 23/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0292 - val_loss: 0.0140\n",
      "Epoch 24/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0282 - val_loss: 0.0142\n",
      "Epoch 25/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0274 - val_loss: 0.0131\n",
      "Epoch 26/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0268 - val_loss: 0.0131\n",
      "Epoch 27/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0263 - val_loss: 0.0129\n",
      "Epoch 28/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0258 - val_loss: 0.0127\n",
      "Epoch 29/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0253 - val_loss: 0.0125\n",
      "Epoch 30/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0120\n",
      "Epoch 31/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0245 - val_loss: 0.0119\n",
      "Epoch 32/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0244 - val_loss: 0.0126\n",
      "Epoch 33/1000\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0242 - val_loss: 0.0115\n",
      "Epoch 34/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0113\n",
      "Epoch 35/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0240 - val_loss: 0.0120\n",
      "Epoch 36/1000\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0126\n"
     ]
    }
   ],
   "source": [
    "sugars_mar_model, sugars_mar_history = autoencoder(sugars_mar_train_df, input_shape = 72, latent_shape = 40, patience = 2, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3130a055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_data (InputLayer)     [(None, 72)]              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 40)                2920      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 72)                2952      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,872\n",
      "Trainable params: 5,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/1000\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.5975 - val_loss: 0.1203\n",
      "Epoch 2/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.3369 - val_loss: 0.0904\n",
      "Epoch 3/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.2500 - val_loss: 0.0700\n",
      "Epoch 4/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.1947 - val_loss: 0.0536\n",
      "Epoch 5/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.1575 - val_loss: 0.0459\n",
      "Epoch 6/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.1305 - val_loss: 0.0380\n",
      "Epoch 7/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.1101 - val_loss: 0.0352\n",
      "Epoch 8/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0954 - val_loss: 0.0325\n",
      "Epoch 9/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0840 - val_loss: 0.0302\n",
      "Epoch 10/1000\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0750 - val_loss: 0.0282\n",
      "Epoch 11/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0673 - val_loss: 0.0285\n",
      "Epoch 12/1000\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.0260\n",
      "Epoch 13/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0551 - val_loss: 0.0254\n",
      "Epoch 14/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0506 - val_loss: 0.0236\n",
      "Epoch 15/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0463 - val_loss: 0.0221\n",
      "Epoch 16/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0429 - val_loss: 0.0213\n",
      "Epoch 17/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0399 - val_loss: 0.0206\n",
      "Epoch 18/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0376 - val_loss: 0.0202\n",
      "Epoch 19/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0356 - val_loss: 0.0186\n",
      "Epoch 20/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0337 - val_loss: 0.0179\n",
      "Epoch 21/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0323 - val_loss: 0.0175\n",
      "Epoch 22/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0313 - val_loss: 0.0168\n",
      "Epoch 23/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0302 - val_loss: 0.0167\n",
      "Epoch 24/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0291 - val_loss: 0.0163\n",
      "Epoch 25/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0285 - val_loss: 0.0143\n",
      "Epoch 26/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0275 - val_loss: 0.0142\n",
      "Epoch 27/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0266 - val_loss: 0.0133\n",
      "Epoch 28/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0261 - val_loss: 0.0122\n",
      "Epoch 29/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0256 - val_loss: 0.0124\n",
      "Epoch 30/1000\n",
      "183/183 [==============================] - 0s 3ms/step - loss: 0.0254 - val_loss: 0.0114\n",
      "Epoch 31/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0250 - val_loss: 0.0121\n",
      "Epoch 32/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0249 - val_loss: 0.0112\n",
      "Epoch 33/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0242 - val_loss: 0.0108\n",
      "Epoch 34/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0237 - val_loss: 0.0114\n",
      "Epoch 35/1000\n",
      "183/183 [==============================] - 0s 2ms/step - loss: 0.0236 - val_loss: 0.0113\n"
     ]
    }
   ],
   "source": [
    "water_mar_model, water_mar_history = autoencoder(water_mar_train_df, input_shape = 72, latent_shape = 40, patience = 2, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a0b342b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_data (InputLayer)     [(None, 72)]              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 40)                2920      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 72)                2952      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,872\n",
      "Trainable params: 5,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 1s 3ms/step - loss: 0.8786 - val_loss: 0.2011\n",
      "Epoch 2/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5347 - val_loss: 0.1472\n",
      "Epoch 3/1000\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4113 - val_loss: 0.1211\n",
      "Epoch 4/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3307 - val_loss: 0.1060\n",
      "Epoch 5/1000\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2711 - val_loss: 0.0953\n",
      "Epoch 6/1000\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2282 - val_loss: 0.0877\n",
      "Epoch 7/1000\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.1949 - val_loss: 0.0823\n",
      "Epoch 8/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1701 - val_loss: 0.0752\n",
      "Epoch 9/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1510 - val_loss: 0.0742\n",
      "Epoch 10/1000\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.1355 - val_loss: 0.0687\n",
      "Epoch 11/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1219 - val_loss: 0.0660\n",
      "Epoch 12/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1104 - val_loss: 0.0620\n",
      "Epoch 13/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1006 - val_loss: 0.0607\n",
      "Epoch 14/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0917 - val_loss: 0.0585\n",
      "Epoch 15/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0843 - val_loss: 0.0543\n",
      "Epoch 16/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0775 - val_loss: 0.0524\n",
      "Epoch 17/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0716 - val_loss: 0.0518\n",
      "Epoch 18/1000\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.0662 - val_loss: 0.0504\n",
      "Epoch 19/1000\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.0481\n",
      "Epoch 20/1000\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.0581 - val_loss: 0.0474\n",
      "Epoch 21/1000\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.0471\n",
      "Epoch 22/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0534 - val_loss: 0.0468\n",
      "Epoch 23/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0504 - val_loss: 0.0433\n",
      "Epoch 24/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0476 - val_loss: 0.0435\n",
      "Epoch 25/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0456 - val_loss: 0.0428\n",
      "Epoch 26/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0446 - val_loss: 0.0421\n",
      "Epoch 27/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0430 - val_loss: 0.0414\n",
      "Epoch 28/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.0400\n",
      "Epoch 29/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0408 - val_loss: 0.0395\n",
      "Epoch 30/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0406 - val_loss: 0.0395\n",
      "Epoch 31/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0396 - val_loss: 0.0385\n",
      "Epoch 32/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.0402\n",
      "Epoch 33/1000\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0388 - val_loss: 0.0387\n"
     ]
    }
   ],
   "source": [
    "water_mcar_model, water_mcar_history = autoencoder(water_mcar_train_df, input_shape = 72, latent_shape = 40, patience = 2, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9324cd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_data (InputLayer)     [(None, 72)]              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 40)                2920      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 72)                2952      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,872\n",
      "Trainable params: 5,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/1000\n",
      "162/162 [==============================] - 1s 3ms/step - loss: 0.6495 - val_loss: 0.1221\n",
      "Epoch 2/1000\n",
      "162/162 [==============================] - 0s 3ms/step - loss: 0.3735 - val_loss: 0.0919\n",
      "Epoch 3/1000\n",
      "162/162 [==============================] - 0s 3ms/step - loss: 0.2765 - val_loss: 0.0735\n",
      "Epoch 4/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.2194 - val_loss: 0.0604\n",
      "Epoch 5/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.1794 - val_loss: 0.0511\n",
      "Epoch 6/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.1502 - val_loss: 0.0437\n",
      "Epoch 7/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.1277 - val_loss: 0.0389\n",
      "Epoch 8/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.1104 - val_loss: 0.0356\n",
      "Epoch 9/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0973 - val_loss: 0.0338\n",
      "Epoch 10/1000\n",
      "162/162 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 0.0309\n",
      "Epoch 11/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0775 - val_loss: 0.0289\n",
      "Epoch 12/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0698 - val_loss: 0.0269\n",
      "Epoch 13/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0637 - val_loss: 0.0249\n",
      "Epoch 14/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0232\n",
      "Epoch 15/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0529 - val_loss: 0.0218\n",
      "Epoch 16/1000\n",
      "162/162 [==============================] - 0s 3ms/step - loss: 0.0487 - val_loss: 0.0202\n",
      "Epoch 17/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0455 - val_loss: 0.0195\n",
      "Epoch 18/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0427 - val_loss: 0.0189\n",
      "Epoch 19/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0404 - val_loss: 0.0174\n",
      "Epoch 20/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.0164\n",
      "Epoch 21/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0370 - val_loss: 0.0160\n",
      "Epoch 22/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0352 - val_loss: 0.0149\n",
      "Epoch 23/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.0147\n",
      "Epoch 24/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0329 - val_loss: 0.0145\n",
      "Epoch 25/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0319 - val_loss: 0.0142\n",
      "Epoch 26/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0314 - val_loss: 0.0138\n",
      "Epoch 27/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0314 - val_loss: 0.0133\n",
      "Epoch 28/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0299 - val_loss: 0.0127\n",
      "Epoch 29/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0287 - val_loss: 0.0125\n",
      "Epoch 30/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0278 - val_loss: 0.0122\n",
      "Epoch 31/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0273 - val_loss: 0.0115\n",
      "Epoch 32/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0267 - val_loss: 0.0110\n",
      "Epoch 33/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0260 - val_loss: 0.0110\n",
      "Epoch 34/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0257 - val_loss: 0.0107\n",
      "Epoch 35/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0255 - val_loss: 0.0107\n",
      "Epoch 36/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0251 - val_loss: 0.0109\n",
      "Epoch 37/1000\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.0252 - val_loss: 0.0107\n"
     ]
    }
   ],
   "source": [
    "lutein_mar_model, lutein_mar_history = autoencoder(lutein_mar_train_df, input_shape = 72, latent_shape = 40, patience = 2, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34d53a3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_data (InputLayer)     [(None, 72)]              0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 40)                2920      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 72)                2952      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,872\n",
      "Trainable params: 5,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 1s 4ms/step - loss: 0.9640 - val_loss: 0.1636\n",
      "Epoch 2/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.5907 - val_loss: 0.1110\n",
      "Epoch 3/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.4545 - val_loss: 0.0912\n",
      "Epoch 4/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.3628 - val_loss: 0.0772\n",
      "Epoch 5/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2994 - val_loss: 0.0670\n",
      "Epoch 6/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.2542 - val_loss: 0.0592\n",
      "Epoch 7/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.2196 - val_loss: 0.0548\n",
      "Epoch 8/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.1916 - val_loss: 0.0501\n",
      "Epoch 9/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1680 - val_loss: 0.0484\n",
      "Epoch 10/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1482 - val_loss: 0.0436\n",
      "Epoch 11/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.1332 - val_loss: 0.0416\n",
      "Epoch 12/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1188 - val_loss: 0.0407\n",
      "Epoch 13/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.1085 - val_loss: 0.0375\n",
      "Epoch 14/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0995 - val_loss: 0.0377\n",
      "Epoch 15/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0913 - val_loss: 0.0359\n",
      "Epoch 16/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0843 - val_loss: 0.0336\n",
      "Epoch 17/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0783 - val_loss: 0.0333\n",
      "Epoch 18/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0324\n",
      "Epoch 19/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0676 - val_loss: 0.0289\n",
      "Epoch 20/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0626 - val_loss: 0.0280\n",
      "Epoch 21/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.0263\n",
      "Epoch 22/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0557 - val_loss: 0.0246\n",
      "Epoch 23/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.0233\n",
      "Epoch 24/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.0226\n",
      "Epoch 25/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.0209\n",
      "Epoch 26/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0465 - val_loss: 0.0204\n",
      "Epoch 27/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0454 - val_loss: 0.0188\n",
      "Epoch 28/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0430 - val_loss: 0.0186\n",
      "Epoch 29/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0420 - val_loss: 0.0177\n",
      "Epoch 30/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.0172\n",
      "Epoch 31/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.0159\n",
      "Epoch 32/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.0153\n",
      "Epoch 33/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.0141\n",
      "Epoch 34/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.0141\n",
      "Epoch 35/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.0133\n",
      "Epoch 36/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.0132\n",
      "Epoch 37/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.0127\n",
      "Epoch 38/1000\n",
      "140/140 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0142\n",
      "Epoch 39/1000\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 0.0361 - val_loss: 0.0127\n"
     ]
    }
   ],
   "source": [
    "lutein_mcar_model, lutein_mcar_history = autoencoder(lutein_mcar_train_df, input_shape = 72, latent_shape = 40, patience = 2, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6422fcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 14:26:47         1884\n",
      "metadata.json                                  2022-11-28 14:26:47           64\n",
      "variables.h5                                   2022-11-28 14:26:47        36504\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "lutein_mcar_model.save(f'{model_dir}/lutein_mcar_model.h5')\n",
    "with open(f'{model_dir}/lutein_mcar_model.pkl', 'wb') as file_pi:\n",
    "    pickle.dump(lutein_mcar_model.history, file_pi)\n",
    "# json.dump(lutein_mcar_model.history, open(f'{model_dir}/lutein_mcar_model.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85139c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f890de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8737c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 14:43:02         1878\n",
      "metadata.json                                  2022-11-28 14:43:02           64\n",
      "variables.h5                                   2022-11-28 14:43:02        36504\n"
     ]
    }
   ],
   "source": [
    "save_autoencoder(model = sugars_mcar_model, model_name = 'sugars_mcar_model',path = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "41e775a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 14:43:14         1878\n",
      "metadata.json                                  2022-11-28 14:43:14           64\n",
      "variables.h5                                   2022-11-28 14:43:14        36504\n"
     ]
    }
   ],
   "source": [
    "save_autoencoder(model = sugars_mar_model, model_name = 'sugars_mar_model',path = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eadff1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 14:43:19         1878\n",
      "metadata.json                                  2022-11-28 14:43:19           64\n",
      "variables.h5                                   2022-11-28 14:43:19        36504\n"
     ]
    }
   ],
   "source": [
    "save_autoencoder(model = water_mar_model, model_name = 'water_mar_model',path = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5ca99960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 14:43:25         1884\n",
      "metadata.json                                  2022-11-28 14:43:25           64\n",
      "variables.h5                                   2022-11-28 14:43:25        36504\n"
     ]
    }
   ],
   "source": [
    "save_autoencoder(model = water_mcar_model, model_name = 'water_mcar_model',path = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a20b318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 14:43:30         1884\n",
      "metadata.json                                  2022-11-28 14:43:30           64\n",
      "variables.h5                                   2022-11-28 14:43:30        36504\n"
     ]
    }
   ],
   "source": [
    "save_autoencoder(model = lutein_mar_model, model_name = 'lutein_mar_model',path = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1cd06d52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 14:43:34         1884\n",
      "metadata.json                                  2022-11-28 14:43:34           64\n",
      "variables.h5                                   2022-11-28 14:43:34        36504\n"
     ]
    }
   ],
   "source": [
    "save_autoencoder(model = lutein_mcar_model, model_name = 'lutein_mcar_model',path = model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "437e2ceb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 14:43:34         1884\n",
      "metadata.json                                  2022-11-28 14:43:34           64\n",
      "variables.h5                                   2022-11-28 14:43:34        36504\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dense_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...vars\n"
     ]
    }
   ],
   "source": [
    "tes = pd.read_pickle(f'{model_dir}/lutein_mcar_model_history.pkl').history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "180dcc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = keras.models.load_model(f'{model_dir}/lutein_mcar_model.h5')\n",
    "test.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8078e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_water_mcar = water_mcar_train_df.water_mcar.max()\n",
    "min_water_mcar = water_mcar_train_df.water_mcar.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba50750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_mcar_test_df['water_mcar'] = 0#np.random.uniform(min_water_mcar, max_water_mcar, size=len(water_mcar_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "730bb124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_fat</th>\n",
       "      <th>saturated_fat</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>sodium</th>\n",
       "      <th>choline</th>\n",
       "      <th>folate</th>\n",
       "      <th>folic_acid</th>\n",
       "      <th>niacin</th>\n",
       "      <th>pantothenic_acid</th>\n",
       "      <th>riboflavin</th>\n",
       "      <th>thiamin</th>\n",
       "      <th>vitamin_a</th>\n",
       "      <th>vitamin_a_rae</th>\n",
       "      <th>carotene_alpha</th>\n",
       "      <th>carotene_beta</th>\n",
       "      <th>cryptoxanthin_beta</th>\n",
       "      <th>lutein_zeaxanthin</th>\n",
       "      <th>vitamin_b12</th>\n",
       "      <th>vitamin_b6</th>\n",
       "      <th>vitamin_c</th>\n",
       "      <th>vitamin_d</th>\n",
       "      <th>vitamin_e</th>\n",
       "      <th>tocopherol_alpha</th>\n",
       "      <th>vitamin_k</th>\n",
       "      <th>calcium</th>\n",
       "      <th>copper</th>\n",
       "      <th>irom</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>manganese</th>\n",
       "      <th>phosphorous</th>\n",
       "      <th>potassium</th>\n",
       "      <th>selenium</th>\n",
       "      <th>zink</th>\n",
       "      <th>protein</th>\n",
       "      <th>alanine</th>\n",
       "      <th>arginine</th>\n",
       "      <th>aspartic_acid</th>\n",
       "      <th>cystine</th>\n",
       "      <th>glutamic_acid</th>\n",
       "      <th>glycine</th>\n",
       "      <th>histidine</th>\n",
       "      <th>hydroxyproline</th>\n",
       "      <th>isoleucine</th>\n",
       "      <th>leucine</th>\n",
       "      <th>lysine</th>\n",
       "      <th>methionine</th>\n",
       "      <th>phenylalanine</th>\n",
       "      <th>proline</th>\n",
       "      <th>serine</th>\n",
       "      <th>threonine</th>\n",
       "      <th>tryptophan</th>\n",
       "      <th>tyrosine</th>\n",
       "      <th>valine</th>\n",
       "      <th>carbohydrate</th>\n",
       "      <th>fiber</th>\n",
       "      <th>sugars</th>\n",
       "      <th>fructose</th>\n",
       "      <th>galactose</th>\n",
       "      <th>glucose</th>\n",
       "      <th>lactose</th>\n",
       "      <th>maltose</th>\n",
       "      <th>sucrose</th>\n",
       "      <th>fat</th>\n",
       "      <th>saturated_fatty_acids</th>\n",
       "      <th>monounsaturated_fatty_acids</th>\n",
       "      <th>polyunsaturated_fatty_acids</th>\n",
       "      <th>fatty_acids_total_trans</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>ash</th>\n",
       "      <th>caffeine</th>\n",
       "      <th>theobromine</th>\n",
       "      <th>water_mcar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.619342</td>\n",
       "      <td>-0.551556</td>\n",
       "      <td>-0.366113</td>\n",
       "      <td>-0.419401</td>\n",
       "      <td>-0.264566</td>\n",
       "      <td>-0.377104</td>\n",
       "      <td>-0.198124</td>\n",
       "      <td>-0.121463</td>\n",
       "      <td>0.295053</td>\n",
       "      <td>0.016684</td>\n",
       "      <td>0.315431</td>\n",
       "      <td>-0.165265</td>\n",
       "      <td>-0.119859</td>\n",
       "      <td>-0.06443</td>\n",
       "      <td>-0.101149</td>\n",
       "      <td>-0.039304</td>\n",
       "      <td>-0.039796</td>\n",
       "      <td>-0.312098</td>\n",
       "      <td>0.387865</td>\n",
       "      <td>-0.120460</td>\n",
       "      <td>-0.125774</td>\n",
       "      <td>-0.224649</td>\n",
       "      <td>-0.224649</td>\n",
       "      <td>-0.106650</td>\n",
       "      <td>0.565155</td>\n",
       "      <td>1.057292</td>\n",
       "      <td>0.776284</td>\n",
       "      <td>2.507023</td>\n",
       "      <td>1.367768</td>\n",
       "      <td>1.442629</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>-0.355989</td>\n",
       "      <td>0.378058</td>\n",
       "      <td>0.005471</td>\n",
       "      <td>0.247723</td>\n",
       "      <td>-0.195034</td>\n",
       "      <td>-0.083019</td>\n",
       "      <td>0.785573</td>\n",
       "      <td>0.862210</td>\n",
       "      <td>-0.076787</td>\n",
       "      <td>-0.062212</td>\n",
       "      <td>-0.367017</td>\n",
       "      <td>0.059323</td>\n",
       "      <td>0.257674</td>\n",
       "      <td>-0.452719</td>\n",
       "      <td>0.554252</td>\n",
       "      <td>0.543382</td>\n",
       "      <td>0.213058</td>\n",
       "      <td>0.379865</td>\n",
       "      <td>0.157415</td>\n",
       "      <td>0.193964</td>\n",
       "      <td>0.252622</td>\n",
       "      <td>0.283687</td>\n",
       "      <td>1.776759</td>\n",
       "      <td>1.304084</td>\n",
       "      <td>-0.352894</td>\n",
       "      <td>0.311358</td>\n",
       "      <td>-0.062457</td>\n",
       "      <td>0.372412</td>\n",
       "      <td>-0.07869</td>\n",
       "      <td>-0.150615</td>\n",
       "      <td>0.023213</td>\n",
       "      <td>-0.620268</td>\n",
       "      <td>-0.544198</td>\n",
       "      <td>-0.549419</td>\n",
       "      <td>-0.273112</td>\n",
       "      <td>-0.366113</td>\n",
       "      <td>-0.024432</td>\n",
       "      <td>0.265352</td>\n",
       "      <td>-0.032626</td>\n",
       "      <td>-0.083373</td>\n",
       "      <td>-0.003348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.643570</td>\n",
       "      <td>-0.435218</td>\n",
       "      <td>-0.358304</td>\n",
       "      <td>-0.373641</td>\n",
       "      <td>-0.361717</td>\n",
       "      <td>-0.346635</td>\n",
       "      <td>-0.198124</td>\n",
       "      <td>-0.838286</td>\n",
       "      <td>-0.244720</td>\n",
       "      <td>-0.411286</td>\n",
       "      <td>-0.437932</td>\n",
       "      <td>-0.154655</td>\n",
       "      <td>-0.105630</td>\n",
       "      <td>-0.06443</td>\n",
       "      <td>-0.105163</td>\n",
       "      <td>0.004479</td>\n",
       "      <td>-0.109427</td>\n",
       "      <td>-0.283870</td>\n",
       "      <td>-0.564302</td>\n",
       "      <td>-0.070570</td>\n",
       "      <td>-0.125774</td>\n",
       "      <td>-0.241400</td>\n",
       "      <td>-0.241400</td>\n",
       "      <td>-0.137364</td>\n",
       "      <td>-0.136664</td>\n",
       "      <td>-0.267279</td>\n",
       "      <td>-0.475648</td>\n",
       "      <td>-0.467465</td>\n",
       "      <td>-0.075670</td>\n",
       "      <td>-0.778567</td>\n",
       "      <td>-0.580093</td>\n",
       "      <td>-0.455583</td>\n",
       "      <td>-0.510558</td>\n",
       "      <td>-1.183777</td>\n",
       "      <td>-0.859682</td>\n",
       "      <td>-0.839409</td>\n",
       "      <td>-0.841020</td>\n",
       "      <td>-0.883861</td>\n",
       "      <td>-0.933870</td>\n",
       "      <td>-0.779530</td>\n",
       "      <td>-0.854175</td>\n",
       "      <td>-0.367017</td>\n",
       "      <td>-0.891171</td>\n",
       "      <td>-0.904661</td>\n",
       "      <td>-0.834197</td>\n",
       "      <td>-0.856819</td>\n",
       "      <td>-0.915290</td>\n",
       "      <td>-0.882456</td>\n",
       "      <td>-0.907343</td>\n",
       "      <td>-0.882938</td>\n",
       "      <td>-0.868764</td>\n",
       "      <td>-0.882973</td>\n",
       "      <td>-0.905697</td>\n",
       "      <td>0.257549</td>\n",
       "      <td>-0.189534</td>\n",
       "      <td>1.322267</td>\n",
       "      <td>-0.162020</td>\n",
       "      <td>-0.062457</td>\n",
       "      <td>-0.142311</td>\n",
       "      <td>-0.07869</td>\n",
       "      <td>-0.173220</td>\n",
       "      <td>-0.156322</td>\n",
       "      <td>-0.643289</td>\n",
       "      <td>-0.440772</td>\n",
       "      <td>-0.557515</td>\n",
       "      <td>-0.454119</td>\n",
       "      <td>-0.358304</td>\n",
       "      <td>-0.024432</td>\n",
       "      <td>-0.642443</td>\n",
       "      <td>-0.032626</td>\n",
       "      <td>-0.083373</td>\n",
       "      <td>1.408102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.746538</td>\n",
       "      <td>-0.595182</td>\n",
       "      <td>-0.366113</td>\n",
       "      <td>-0.395175</td>\n",
       "      <td>0.296750</td>\n",
       "      <td>0.057075</td>\n",
       "      <td>-0.198124</td>\n",
       "      <td>-0.741841</td>\n",
       "      <td>0.088316</td>\n",
       "      <td>-0.502818</td>\n",
       "      <td>-0.390198</td>\n",
       "      <td>-0.167846</td>\n",
       "      <td>-0.119859</td>\n",
       "      <td>-0.06443</td>\n",
       "      <td>-0.106167</td>\n",
       "      <td>-0.039304</td>\n",
       "      <td>-0.116508</td>\n",
       "      <td>-0.312098</td>\n",
       "      <td>-0.230317</td>\n",
       "      <td>0.925066</td>\n",
       "      <td>-0.125774</td>\n",
       "      <td>-0.224649</td>\n",
       "      <td>-0.224649</td>\n",
       "      <td>0.113204</td>\n",
       "      <td>-0.314904</td>\n",
       "      <td>-0.248647</td>\n",
       "      <td>-0.428847</td>\n",
       "      <td>-0.349161</td>\n",
       "      <td>-0.053148</td>\n",
       "      <td>-0.755727</td>\n",
       "      <td>0.063794</td>\n",
       "      <td>-0.486491</td>\n",
       "      <td>-0.569799</td>\n",
       "      <td>-1.103844</td>\n",
       "      <td>-0.687716</td>\n",
       "      <td>-0.732221</td>\n",
       "      <td>-0.677402</td>\n",
       "      <td>-0.742383</td>\n",
       "      <td>-0.796040</td>\n",
       "      <td>-0.674929</td>\n",
       "      <td>-0.706833</td>\n",
       "      <td>-0.367017</td>\n",
       "      <td>-0.756470</td>\n",
       "      <td>-0.789298</td>\n",
       "      <td>-0.614036</td>\n",
       "      <td>-0.790881</td>\n",
       "      <td>-0.779454</td>\n",
       "      <td>-0.765315</td>\n",
       "      <td>-0.729369</td>\n",
       "      <td>-0.727905</td>\n",
       "      <td>-0.715853</td>\n",
       "      <td>-0.756520</td>\n",
       "      <td>-0.688973</td>\n",
       "      <td>-0.646582</td>\n",
       "      <td>-0.033484</td>\n",
       "      <td>-0.347678</td>\n",
       "      <td>0.814953</td>\n",
       "      <td>-0.062457</td>\n",
       "      <td>0.520483</td>\n",
       "      <td>-0.07869</td>\n",
       "      <td>-0.173220</td>\n",
       "      <td>-0.156322</td>\n",
       "      <td>-0.747491</td>\n",
       "      <td>-0.590602</td>\n",
       "      <td>-0.625578</td>\n",
       "      <td>-0.463069</td>\n",
       "      <td>-0.366113</td>\n",
       "      <td>-0.024432</td>\n",
       "      <td>-0.476551</td>\n",
       "      <td>-0.032626</td>\n",
       "      <td>-0.083373</td>\n",
       "      <td>1.651227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.710196</td>\n",
       "      <td>-0.580640</td>\n",
       "      <td>-0.366113</td>\n",
       "      <td>-0.388446</td>\n",
       "      <td>-0.500247</td>\n",
       "      <td>-0.232377</td>\n",
       "      <td>-0.198124</td>\n",
       "      <td>-0.495732</td>\n",
       "      <td>-0.325912</td>\n",
       "      <td>-0.282648</td>\n",
       "      <td>0.292602</td>\n",
       "      <td>-0.167846</td>\n",
       "      <td>-0.119859</td>\n",
       "      <td>-0.06443</td>\n",
       "      <td>-0.106167</td>\n",
       "      <td>-0.039304</td>\n",
       "      <td>-0.117689</td>\n",
       "      <td>-0.312098</td>\n",
       "      <td>-0.456431</td>\n",
       "      <td>-0.120460</td>\n",
       "      <td>-0.125774</td>\n",
       "      <td>-0.243793</td>\n",
       "      <td>-0.243793</td>\n",
       "      <td>-0.137364</td>\n",
       "      <td>-0.270344</td>\n",
       "      <td>2.032935</td>\n",
       "      <td>-0.164755</td>\n",
       "      <td>-0.197057</td>\n",
       "      <td>0.005816</td>\n",
       "      <td>-0.750017</td>\n",
       "      <td>0.974116</td>\n",
       "      <td>-0.483057</td>\n",
       "      <td>-0.253846</td>\n",
       "      <td>-0.160244</td>\n",
       "      <td>-0.271142</td>\n",
       "      <td>-0.325903</td>\n",
       "      <td>0.762798</td>\n",
       "      <td>0.509692</td>\n",
       "      <td>-0.504827</td>\n",
       "      <td>-0.204961</td>\n",
       "      <td>-0.220078</td>\n",
       "      <td>-0.367017</td>\n",
       "      <td>-0.084863</td>\n",
       "      <td>-0.208130</td>\n",
       "      <td>-0.233572</td>\n",
       "      <td>-0.385363</td>\n",
       "      <td>0.027205</td>\n",
       "      <td>0.237806</td>\n",
       "      <td>0.052889</td>\n",
       "      <td>0.037060</td>\n",
       "      <td>1.057909</td>\n",
       "      <td>-0.007722</td>\n",
       "      <td>0.132847</td>\n",
       "      <td>0.175776</td>\n",
       "      <td>-0.479340</td>\n",
       "      <td>-0.490007</td>\n",
       "      <td>-0.162020</td>\n",
       "      <td>-0.062457</td>\n",
       "      <td>-0.142311</td>\n",
       "      <td>-0.07869</td>\n",
       "      <td>-0.173220</td>\n",
       "      <td>-0.156322</td>\n",
       "      <td>-0.709930</td>\n",
       "      <td>-0.577219</td>\n",
       "      <td>-0.598133</td>\n",
       "      <td>-0.436950</td>\n",
       "      <td>-0.366113</td>\n",
       "      <td>-0.024432</td>\n",
       "      <td>0.094852</td>\n",
       "      <td>-0.032626</td>\n",
       "      <td>-0.083373</td>\n",
       "      <td>-1.192960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.740481</td>\n",
       "      <td>-0.595182</td>\n",
       "      <td>-0.366113</td>\n",
       "      <td>-0.426130</td>\n",
       "      <td>-0.500247</td>\n",
       "      <td>-0.377104</td>\n",
       "      <td>-0.198124</td>\n",
       "      <td>-0.851971</td>\n",
       "      <td>-0.413117</td>\n",
       "      <td>-0.651247</td>\n",
       "      <td>-0.493967</td>\n",
       "      <td>-0.167846</td>\n",
       "      <td>-0.119859</td>\n",
       "      <td>-0.06443</td>\n",
       "      <td>-0.106167</td>\n",
       "      <td>-0.039304</td>\n",
       "      <td>-0.117689</td>\n",
       "      <td>-0.312098</td>\n",
       "      <td>-0.612014</td>\n",
       "      <td>-0.120460</td>\n",
       "      <td>-0.125774</td>\n",
       "      <td>-0.243793</td>\n",
       "      <td>-0.243793</td>\n",
       "      <td>-0.137364</td>\n",
       "      <td>-0.326044</td>\n",
       "      <td>-0.314706</td>\n",
       "      <td>-0.499049</td>\n",
       "      <td>-0.602669</td>\n",
       "      <td>-0.077391</td>\n",
       "      <td>-1.006968</td>\n",
       "      <td>-0.884591</td>\n",
       "      <td>-0.489926</td>\n",
       "      <td>-0.645966</td>\n",
       "      <td>-1.291004</td>\n",
       "      <td>-0.859682</td>\n",
       "      <td>-0.839409</td>\n",
       "      <td>-0.841020</td>\n",
       "      <td>-0.883861</td>\n",
       "      <td>-0.933870</td>\n",
       "      <td>-0.779530</td>\n",
       "      <td>-0.854175</td>\n",
       "      <td>-0.367017</td>\n",
       "      <td>-0.891171</td>\n",
       "      <td>-0.904661</td>\n",
       "      <td>-0.834197</td>\n",
       "      <td>-0.856819</td>\n",
       "      <td>-0.915290</td>\n",
       "      <td>-0.882456</td>\n",
       "      <td>-0.907343</td>\n",
       "      <td>-0.882938</td>\n",
       "      <td>-0.868764</td>\n",
       "      <td>-0.882973</td>\n",
       "      <td>-0.905697</td>\n",
       "      <td>2.547208</td>\n",
       "      <td>0.055687</td>\n",
       "      <td>-0.490007</td>\n",
       "      <td>-0.162020</td>\n",
       "      <td>-0.062457</td>\n",
       "      <td>-0.142311</td>\n",
       "      <td>-0.07869</td>\n",
       "      <td>-0.173220</td>\n",
       "      <td>-0.156322</td>\n",
       "      <td>-0.740221</td>\n",
       "      <td>-0.601075</td>\n",
       "      <td>-0.617482</td>\n",
       "      <td>-0.426356</td>\n",
       "      <td>-0.366113</td>\n",
       "      <td>-0.024432</td>\n",
       "      <td>-0.227714</td>\n",
       "      <td>-0.032626</td>\n",
       "      <td>-0.083373</td>\n",
       "      <td>1.330437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7166</th>\n",
       "      <td>-0.213525</td>\n",
       "      <td>-0.115289</td>\n",
       "      <td>0.321081</td>\n",
       "      <td>-0.357491</td>\n",
       "      <td>1.439172</td>\n",
       "      <td>-0.308549</td>\n",
       "      <td>-0.198124</td>\n",
       "      <td>0.961156</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>-0.292543</td>\n",
       "      <td>-0.338314</td>\n",
       "      <td>-0.167846</td>\n",
       "      <td>-0.119859</td>\n",
       "      <td>-0.06443</td>\n",
       "      <td>-0.106167</td>\n",
       "      <td>-0.039304</td>\n",
       "      <td>-0.117689</td>\n",
       "      <td>0.072238</td>\n",
       "      <td>0.609829</td>\n",
       "      <td>-0.120460</td>\n",
       "      <td>-0.081150</td>\n",
       "      <td>-0.143288</td>\n",
       "      <td>-0.143288</td>\n",
       "      <td>-0.113116</td>\n",
       "      <td>-0.348324</td>\n",
       "      <td>-0.163956</td>\n",
       "      <td>-0.178126</td>\n",
       "      <td>-0.197057</td>\n",
       "      <td>-0.075670</td>\n",
       "      <td>0.243526</td>\n",
       "      <td>0.231902</td>\n",
       "      <td>0.609044</td>\n",
       "      <td>0.851987</td>\n",
       "      <td>1.466686</td>\n",
       "      <td>1.690166</td>\n",
       "      <td>1.440208</td>\n",
       "      <td>1.541135</td>\n",
       "      <td>1.698103</td>\n",
       "      <td>1.343811</td>\n",
       "      <td>1.758889</td>\n",
       "      <td>1.521714</td>\n",
       "      <td>3.132008</td>\n",
       "      <td>1.550517</td>\n",
       "      <td>1.544079</td>\n",
       "      <td>1.591637</td>\n",
       "      <td>1.572992</td>\n",
       "      <td>1.419004</td>\n",
       "      <td>1.343219</td>\n",
       "      <td>1.398041</td>\n",
       "      <td>1.422158</td>\n",
       "      <td>0.553304</td>\n",
       "      <td>1.351026</td>\n",
       "      <td>1.528554</td>\n",
       "      <td>-0.823283</td>\n",
       "      <td>-0.479340</td>\n",
       "      <td>-0.490007</td>\n",
       "      <td>-0.162020</td>\n",
       "      <td>-0.062457</td>\n",
       "      <td>-0.142311</td>\n",
       "      <td>-0.07869</td>\n",
       "      <td>-0.173220</td>\n",
       "      <td>-0.156322</td>\n",
       "      <td>-0.216185</td>\n",
       "      <td>-0.108383</td>\n",
       "      <td>-0.134591</td>\n",
       "      <td>-0.406265</td>\n",
       "      <td>0.321081</td>\n",
       "      <td>-0.024432</td>\n",
       "      <td>-0.319876</td>\n",
       "      <td>-0.032626</td>\n",
       "      <td>-0.083373</td>\n",
       "      <td>-0.744752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7167</th>\n",
       "      <td>0.628393</td>\n",
       "      <td>0.728159</td>\n",
       "      <td>0.570970</td>\n",
       "      <td>-0.338648</td>\n",
       "      <td>-0.500247</td>\n",
       "      <td>-0.232377</td>\n",
       "      <td>-0.198124</td>\n",
       "      <td>0.583847</td>\n",
       "      <td>0.045465</td>\n",
       "      <td>-0.032792</td>\n",
       "      <td>-0.348691</td>\n",
       "      <td>-0.167846</td>\n",
       "      <td>-0.119859</td>\n",
       "      <td>-0.06443</td>\n",
       "      <td>-0.106167</td>\n",
       "      <td>-0.039304</td>\n",
       "      <td>-0.117689</td>\n",
       "      <td>0.250292</td>\n",
       "      <td>-0.383826</td>\n",
       "      <td>-0.120460</td>\n",
       "      <td>-0.125774</td>\n",
       "      <td>-0.243793</td>\n",
       "      <td>-0.243793</td>\n",
       "      <td>-0.137364</td>\n",
       "      <td>-0.298194</td>\n",
       "      <td>-0.075877</td>\n",
       "      <td>-0.092881</td>\n",
       "      <td>-0.163256</td>\n",
       "      <td>-0.073793</td>\n",
       "      <td>0.192135</td>\n",
       "      <td>0.101856</td>\n",
       "      <td>0.794495</td>\n",
       "      <td>1.114340</td>\n",
       "      <td>1.740603</td>\n",
       "      <td>1.912537</td>\n",
       "      <td>1.462643</td>\n",
       "      <td>1.689038</td>\n",
       "      <td>1.740546</td>\n",
       "      <td>1.485931</td>\n",
       "      <td>1.458345</td>\n",
       "      <td>1.737465</td>\n",
       "      <td>-0.367017</td>\n",
       "      <td>1.954619</td>\n",
       "      <td>1.728007</td>\n",
       "      <td>1.951808</td>\n",
       "      <td>1.774102</td>\n",
       "      <td>1.730382</td>\n",
       "      <td>1.268974</td>\n",
       "      <td>1.484959</td>\n",
       "      <td>1.832179</td>\n",
       "      <td>1.906562</td>\n",
       "      <td>1.708069</td>\n",
       "      <td>2.003614</td>\n",
       "      <td>-0.823283</td>\n",
       "      <td>-0.479340</td>\n",
       "      <td>-0.490007</td>\n",
       "      <td>-0.162020</td>\n",
       "      <td>-0.062457</td>\n",
       "      <td>-0.142311</td>\n",
       "      <td>-0.07869</td>\n",
       "      <td>-0.173220</td>\n",
       "      <td>-0.156322</td>\n",
       "      <td>0.607731</td>\n",
       "      <td>0.730226</td>\n",
       "      <td>0.691223</td>\n",
       "      <td>-0.176489</td>\n",
       "      <td>0.570970</td>\n",
       "      <td>-0.024432</td>\n",
       "      <td>-0.310660</td>\n",
       "      <td>-0.032626</td>\n",
       "      <td>-0.083373</td>\n",
       "      <td>0.690429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7170</th>\n",
       "      <td>0.446684</td>\n",
       "      <td>0.655448</td>\n",
       "      <td>0.305463</td>\n",
       "      <td>-0.339994</td>\n",
       "      <td>0.863463</td>\n",
       "      <td>-0.331401</td>\n",
       "      <td>-0.198124</td>\n",
       "      <td>0.157880</td>\n",
       "      <td>0.147706</td>\n",
       "      <td>-0.186169</td>\n",
       "      <td>-0.356992</td>\n",
       "      <td>-0.160964</td>\n",
       "      <td>-0.111559</td>\n",
       "      <td>-0.06443</td>\n",
       "      <td>-0.106167</td>\n",
       "      <td>-0.039304</td>\n",
       "      <td>-0.117689</td>\n",
       "      <td>0.315434</td>\n",
       "      <td>0.128560</td>\n",
       "      <td>-0.120460</td>\n",
       "      <td>-0.081150</td>\n",
       "      <td>-0.219863</td>\n",
       "      <td>-0.219863</td>\n",
       "      <td>-0.111499</td>\n",
       "      <td>-0.353894</td>\n",
       "      <td>-0.180894</td>\n",
       "      <td>-0.087867</td>\n",
       "      <td>-0.247758</td>\n",
       "      <td>-0.075514</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>0.171637</td>\n",
       "      <td>0.413290</td>\n",
       "      <td>1.788559</td>\n",
       "      <td>1.141105</td>\n",
       "      <td>1.249873</td>\n",
       "      <td>1.207136</td>\n",
       "      <td>1.226842</td>\n",
       "      <td>0.969494</td>\n",
       "      <td>1.181847</td>\n",
       "      <td>0.897034</td>\n",
       "      <td>1.271759</td>\n",
       "      <td>1.635792</td>\n",
       "      <td>1.121751</td>\n",
       "      <td>1.279615</td>\n",
       "      <td>1.378577</td>\n",
       "      <td>1.480679</td>\n",
       "      <td>1.067919</td>\n",
       "      <td>0.808661</td>\n",
       "      <td>1.075205</td>\n",
       "      <td>1.356881</td>\n",
       "      <td>1.279629</td>\n",
       "      <td>1.249367</td>\n",
       "      <td>1.048292</td>\n",
       "      <td>-0.823283</td>\n",
       "      <td>-0.479340</td>\n",
       "      <td>-0.490007</td>\n",
       "      <td>-0.162020</td>\n",
       "      <td>-0.062457</td>\n",
       "      <td>-0.142311</td>\n",
       "      <td>-0.07869</td>\n",
       "      <td>-0.173220</td>\n",
       "      <td>-0.156322</td>\n",
       "      <td>0.468392</td>\n",
       "      <td>0.652693</td>\n",
       "      <td>0.670502</td>\n",
       "      <td>-0.322793</td>\n",
       "      <td>0.305463</td>\n",
       "      <td>-0.024432</td>\n",
       "      <td>-0.393606</td>\n",
       "      <td>-0.032626</td>\n",
       "      <td>-0.083373</td>\n",
       "      <td>1.537408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7186</th>\n",
       "      <td>-0.437633</td>\n",
       "      <td>-0.347965</td>\n",
       "      <td>0.125856</td>\n",
       "      <td>-0.339994</td>\n",
       "      <td>1.273656</td>\n",
       "      <td>-0.316166</td>\n",
       "      <td>-0.198124</td>\n",
       "      <td>-0.062380</td>\n",
       "      <td>-0.413117</td>\n",
       "      <td>-0.082268</td>\n",
       "      <td>-0.263600</td>\n",
       "      <td>-0.167846</td>\n",
       "      <td>-0.119859</td>\n",
       "      <td>-0.06443</td>\n",
       "      <td>-0.106167</td>\n",
       "      <td>-0.039304</td>\n",
       "      <td>-0.117689</td>\n",
       "      <td>0.424004</td>\n",
       "      <td>0.055955</td>\n",
       "      <td>-0.120460</td>\n",
       "      <td>-0.125774</td>\n",
       "      <td>-0.205506</td>\n",
       "      <td>-0.205506</td>\n",
       "      <td>-0.137364</td>\n",
       "      <td>-0.392884</td>\n",
       "      <td>-0.114835</td>\n",
       "      <td>-0.009308</td>\n",
       "      <td>-0.213957</td>\n",
       "      <td>-0.077391</td>\n",
       "      <td>0.289206</td>\n",
       "      <td>0.044762</td>\n",
       "      <td>0.361776</td>\n",
       "      <td>1.562879</td>\n",
       "      <td>1.233710</td>\n",
       "      <td>1.460384</td>\n",
       "      <td>1.335513</td>\n",
       "      <td>1.545757</td>\n",
       "      <td>1.082676</td>\n",
       "      <td>1.292862</td>\n",
       "      <td>1.034047</td>\n",
       "      <td>1.342799</td>\n",
       "      <td>1.046731</td>\n",
       "      <td>1.645377</td>\n",
       "      <td>1.587612</td>\n",
       "      <td>1.632219</td>\n",
       "      <td>1.566398</td>\n",
       "      <td>1.431543</td>\n",
       "      <td>0.924152</td>\n",
       "      <td>1.242832</td>\n",
       "      <td>1.558831</td>\n",
       "      <td>1.271983</td>\n",
       "      <td>1.470040</td>\n",
       "      <td>1.521619</td>\n",
       "      <td>-0.823283</td>\n",
       "      <td>-0.479340</td>\n",
       "      <td>-0.490007</td>\n",
       "      <td>-0.162020</td>\n",
       "      <td>-0.062457</td>\n",
       "      <td>-0.142311</td>\n",
       "      <td>-0.07869</td>\n",
       "      <td>-0.173220</td>\n",
       "      <td>-0.156322</td>\n",
       "      <td>-0.436098</td>\n",
       "      <td>-0.343601</td>\n",
       "      <td>-0.272363</td>\n",
       "      <td>-0.398593</td>\n",
       "      <td>0.125856</td>\n",
       "      <td>-0.024432</td>\n",
       "      <td>-0.200066</td>\n",
       "      <td>-0.032626</td>\n",
       "      <td>-0.083373</td>\n",
       "      <td>0.186553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7187</th>\n",
       "      <td>0.991811</td>\n",
       "      <td>1.135341</td>\n",
       "      <td>0.274227</td>\n",
       "      <td>-0.350761</td>\n",
       "      <td>1.025381</td>\n",
       "      <td>-0.331401</td>\n",
       "      <td>-0.198124</td>\n",
       "      <td>-0.146009</td>\n",
       "      <td>-0.172550</td>\n",
       "      <td>-0.230698</td>\n",
       "      <td>-0.327937</td>\n",
       "      <td>-0.167846</td>\n",
       "      <td>-0.119859</td>\n",
       "      <td>-0.06443</td>\n",
       "      <td>-0.106167</td>\n",
       "      <td>-0.039304</td>\n",
       "      <td>-0.117689</td>\n",
       "      <td>0.311091</td>\n",
       "      <td>-0.051916</td>\n",
       "      <td>-0.120460</td>\n",
       "      <td>-0.125774</td>\n",
       "      <td>-0.243793</td>\n",
       "      <td>-0.243793</td>\n",
       "      <td>-0.137364</td>\n",
       "      <td>-0.376174</td>\n",
       "      <td>-0.177506</td>\n",
       "      <td>-0.134668</td>\n",
       "      <td>-0.264659</td>\n",
       "      <td>-0.075357</td>\n",
       "      <td>0.009415</td>\n",
       "      <td>0.108199</td>\n",
       "      <td>0.241576</td>\n",
       "      <td>0.829419</td>\n",
       "      <td>0.878886</td>\n",
       "      <td>1.131275</td>\n",
       "      <td>0.914238</td>\n",
       "      <td>1.039191</td>\n",
       "      <td>0.877534</td>\n",
       "      <td>0.860065</td>\n",
       "      <td>1.010475</td>\n",
       "      <td>1.150728</td>\n",
       "      <td>-0.367017</td>\n",
       "      <td>1.007920</td>\n",
       "      <td>1.010798</td>\n",
       "      <td>1.044784</td>\n",
       "      <td>1.022411</td>\n",
       "      <td>0.900736</td>\n",
       "      <td>0.739366</td>\n",
       "      <td>0.853772</td>\n",
       "      <td>1.099852</td>\n",
       "      <td>1.034972</td>\n",
       "      <td>0.971667</td>\n",
       "      <td>0.972005</td>\n",
       "      <td>-0.823283</td>\n",
       "      <td>-0.479340</td>\n",
       "      <td>-0.490007</td>\n",
       "      <td>-0.162020</td>\n",
       "      <td>-0.062457</td>\n",
       "      <td>-0.142311</td>\n",
       "      <td>-0.07869</td>\n",
       "      <td>-0.173220</td>\n",
       "      <td>-0.156322</td>\n",
       "      <td>0.962137</td>\n",
       "      <td>1.073525</td>\n",
       "      <td>1.032911</td>\n",
       "      <td>-0.278774</td>\n",
       "      <td>0.274227</td>\n",
       "      <td>-0.024432</td>\n",
       "      <td>-0.352133</td>\n",
       "      <td>-0.032626</td>\n",
       "      <td>-0.083373</td>\n",
       "      <td>0.203349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1476 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      total_fat  saturated_fat  cholesterol    sodium   choline    folate  \\\n",
       "1     -0.619342      -0.551556    -0.366113 -0.419401 -0.264566 -0.377104   \n",
       "2     -0.643570      -0.435218    -0.358304 -0.373641 -0.361717 -0.346635   \n",
       "3     -0.746538      -0.595182    -0.366113 -0.395175  0.296750  0.057075   \n",
       "13    -0.710196      -0.580640    -0.366113 -0.388446 -0.500247 -0.232377   \n",
       "20    -0.740481      -0.595182    -0.366113 -0.426130 -0.500247 -0.377104   \n",
       "...         ...            ...          ...       ...       ...       ...   \n",
       "7166  -0.213525      -0.115289     0.321081 -0.357491  1.439172 -0.308549   \n",
       "7167   0.628393       0.728159     0.570970 -0.338648 -0.500247 -0.232377   \n",
       "7170   0.446684       0.655448     0.305463 -0.339994  0.863463 -0.331401   \n",
       "7186  -0.437633      -0.347965     0.125856 -0.339994  1.273656 -0.316166   \n",
       "7187   0.991811       1.135341     0.274227 -0.350761  1.025381 -0.331401   \n",
       "\n",
       "      folic_acid    niacin  pantothenic_acid  riboflavin   thiamin  vitamin_a  \\\n",
       "1      -0.198124 -0.121463          0.295053    0.016684  0.315431  -0.165265   \n",
       "2      -0.198124 -0.838286         -0.244720   -0.411286 -0.437932  -0.154655   \n",
       "3      -0.198124 -0.741841          0.088316   -0.502818 -0.390198  -0.167846   \n",
       "13     -0.198124 -0.495732         -0.325912   -0.282648  0.292602  -0.167846   \n",
       "20     -0.198124 -0.851971         -0.413117   -0.651247 -0.493967  -0.167846   \n",
       "...          ...       ...               ...         ...       ...        ...   \n",
       "7166   -0.198124  0.961156          0.004869   -0.292543 -0.338314  -0.167846   \n",
       "7167   -0.198124  0.583847          0.045465   -0.032792 -0.348691  -0.167846   \n",
       "7170   -0.198124  0.157880          0.147706   -0.186169 -0.356992  -0.160964   \n",
       "7186   -0.198124 -0.062380         -0.413117   -0.082268 -0.263600  -0.167846   \n",
       "7187   -0.198124 -0.146009         -0.172550   -0.230698 -0.327937  -0.167846   \n",
       "\n",
       "      vitamin_a_rae  carotene_alpha  carotene_beta  cryptoxanthin_beta  \\\n",
       "1         -0.119859        -0.06443      -0.101149           -0.039304   \n",
       "2         -0.105630        -0.06443      -0.105163            0.004479   \n",
       "3         -0.119859        -0.06443      -0.106167           -0.039304   \n",
       "13        -0.119859        -0.06443      -0.106167           -0.039304   \n",
       "20        -0.119859        -0.06443      -0.106167           -0.039304   \n",
       "...             ...             ...            ...                 ...   \n",
       "7166      -0.119859        -0.06443      -0.106167           -0.039304   \n",
       "7167      -0.119859        -0.06443      -0.106167           -0.039304   \n",
       "7170      -0.111559        -0.06443      -0.106167           -0.039304   \n",
       "7186      -0.119859        -0.06443      -0.106167           -0.039304   \n",
       "7187      -0.119859        -0.06443      -0.106167           -0.039304   \n",
       "\n",
       "      lutein_zeaxanthin  vitamin_b12  vitamin_b6  vitamin_c  vitamin_d  \\\n",
       "1             -0.039796    -0.312098    0.387865  -0.120460  -0.125774   \n",
       "2             -0.109427    -0.283870   -0.564302  -0.070570  -0.125774   \n",
       "3             -0.116508    -0.312098   -0.230317   0.925066  -0.125774   \n",
       "13            -0.117689    -0.312098   -0.456431  -0.120460  -0.125774   \n",
       "20            -0.117689    -0.312098   -0.612014  -0.120460  -0.125774   \n",
       "...                 ...          ...         ...        ...        ...   \n",
       "7166          -0.117689     0.072238    0.609829  -0.120460  -0.081150   \n",
       "7167          -0.117689     0.250292   -0.383826  -0.120460  -0.125774   \n",
       "7170          -0.117689     0.315434    0.128560  -0.120460  -0.081150   \n",
       "7186          -0.117689     0.424004    0.055955  -0.120460  -0.125774   \n",
       "7187          -0.117689     0.311091   -0.051916  -0.120460  -0.125774   \n",
       "\n",
       "      vitamin_e  tocopherol_alpha  vitamin_k   calcium    copper      irom  \\\n",
       "1     -0.224649         -0.224649  -0.106650  0.565155  1.057292  0.776284   \n",
       "2     -0.241400         -0.241400  -0.137364 -0.136664 -0.267279 -0.475648   \n",
       "3     -0.224649         -0.224649   0.113204 -0.314904 -0.248647 -0.428847   \n",
       "13    -0.243793         -0.243793  -0.137364 -0.270344  2.032935 -0.164755   \n",
       "20    -0.243793         -0.243793  -0.137364 -0.326044 -0.314706 -0.499049   \n",
       "...         ...               ...        ...       ...       ...       ...   \n",
       "7166  -0.143288         -0.143288  -0.113116 -0.348324 -0.163956 -0.178126   \n",
       "7167  -0.243793         -0.243793  -0.137364 -0.298194 -0.075877 -0.092881   \n",
       "7170  -0.219863         -0.219863  -0.111499 -0.353894 -0.180894 -0.087867   \n",
       "7186  -0.205506         -0.205506  -0.137364 -0.392884 -0.114835 -0.009308   \n",
       "7187  -0.243793         -0.243793  -0.137364 -0.376174 -0.177506 -0.134668   \n",
       "\n",
       "      magnesium  manganese  phosphorous  potassium  selenium      zink  \\\n",
       "1      2.507023   1.367768     1.442629   0.469791 -0.355989  0.378058   \n",
       "2     -0.467465  -0.075670    -0.778567  -0.580093 -0.455583 -0.510558   \n",
       "3     -0.349161  -0.053148    -0.755727   0.063794 -0.486491 -0.569799   \n",
       "13    -0.197057   0.005816    -0.750017   0.974116 -0.483057 -0.253846   \n",
       "20    -0.602669  -0.077391    -1.006968  -0.884591 -0.489926 -0.645966   \n",
       "...         ...        ...          ...        ...       ...       ...   \n",
       "7166  -0.197057  -0.075670     0.243526   0.231902  0.609044  0.851987   \n",
       "7167  -0.163256  -0.073793     0.192135   0.101856  0.794495  1.114340   \n",
       "7170  -0.247758  -0.075514     0.089355   0.171637  0.413290  1.788559   \n",
       "7186  -0.213957  -0.077391     0.289206   0.044762  0.361776  1.562879   \n",
       "7187  -0.264659  -0.075357     0.009415   0.108199  0.241576  0.829419   \n",
       "\n",
       "       protein   alanine  arginine  aspartic_acid   cystine  glutamic_acid  \\\n",
       "1     0.005471  0.247723 -0.195034      -0.083019  0.785573       0.862210   \n",
       "2    -1.183777 -0.859682 -0.839409      -0.841020 -0.883861      -0.933870   \n",
       "3    -1.103844 -0.687716 -0.732221      -0.677402 -0.742383      -0.796040   \n",
       "13   -0.160244 -0.271142 -0.325903       0.762798  0.509692      -0.504827   \n",
       "20   -1.291004 -0.859682 -0.839409      -0.841020 -0.883861      -0.933870   \n",
       "...        ...       ...       ...            ...       ...            ...   \n",
       "7166  1.466686  1.690166  1.440208       1.541135  1.698103       1.343811   \n",
       "7167  1.740603  1.912537  1.462643       1.689038  1.740546       1.485931   \n",
       "7170  1.141105  1.249873  1.207136       1.226842  0.969494       1.181847   \n",
       "7186  1.233710  1.460384  1.335513       1.545757  1.082676       1.292862   \n",
       "7187  0.878886  1.131275  0.914238       1.039191  0.877534       0.860065   \n",
       "\n",
       "       glycine  histidine  hydroxyproline  isoleucine   leucine    lysine  \\\n",
       "1    -0.076787  -0.062212       -0.367017    0.059323  0.257674 -0.452719   \n",
       "2    -0.779530  -0.854175       -0.367017   -0.891171 -0.904661 -0.834197   \n",
       "3    -0.674929  -0.706833       -0.367017   -0.756470 -0.789298 -0.614036   \n",
       "13   -0.204961  -0.220078       -0.367017   -0.084863 -0.208130 -0.233572   \n",
       "20   -0.779530  -0.854175       -0.367017   -0.891171 -0.904661 -0.834197   \n",
       "...        ...        ...             ...         ...       ...       ...   \n",
       "7166  1.758889   1.521714        3.132008    1.550517  1.544079  1.591637   \n",
       "7167  1.458345   1.737465       -0.367017    1.954619  1.728007  1.951808   \n",
       "7170  0.897034   1.271759        1.635792    1.121751  1.279615  1.378577   \n",
       "7186  1.034047   1.342799        1.046731    1.645377  1.587612  1.632219   \n",
       "7187  1.010475   1.150728       -0.367017    1.007920  1.010798  1.044784   \n",
       "\n",
       "      methionine  phenylalanine   proline    serine  threonine  tryptophan  \\\n",
       "1       0.554252       0.543382  0.213058  0.379865   0.157415    0.193964   \n",
       "2      -0.856819      -0.915290 -0.882456 -0.907343  -0.882938   -0.868764   \n",
       "3      -0.790881      -0.779454 -0.765315 -0.729369  -0.727905   -0.715853   \n",
       "13     -0.385363       0.027205  0.237806  0.052889   0.037060    1.057909   \n",
       "20     -0.856819      -0.915290 -0.882456 -0.907343  -0.882938   -0.868764   \n",
       "...          ...            ...       ...       ...        ...         ...   \n",
       "7166    1.572992       1.419004  1.343219  1.398041   1.422158    0.553304   \n",
       "7167    1.774102       1.730382  1.268974  1.484959   1.832179    1.906562   \n",
       "7170    1.480679       1.067919  0.808661  1.075205   1.356881    1.279629   \n",
       "7186    1.566398       1.431543  0.924152  1.242832   1.558831    1.271983   \n",
       "7187    1.022411       0.900736  0.739366  0.853772   1.099852    1.034972   \n",
       "\n",
       "      tyrosine    valine  carbohydrate     fiber    sugars  fructose  \\\n",
       "1     0.252622  0.283687      1.776759  1.304084 -0.352894  0.311358   \n",
       "2    -0.882973 -0.905697      0.257549 -0.189534  1.322267 -0.162020   \n",
       "3    -0.756520 -0.688973     -0.646582 -0.033484 -0.347678  0.814953   \n",
       "13   -0.007722  0.132847      0.175776 -0.479340 -0.490007 -0.162020   \n",
       "20   -0.882973 -0.905697      2.547208  0.055687 -0.490007 -0.162020   \n",
       "...        ...       ...           ...       ...       ...       ...   \n",
       "7166  1.351026  1.528554     -0.823283 -0.479340 -0.490007 -0.162020   \n",
       "7167  1.708069  2.003614     -0.823283 -0.479340 -0.490007 -0.162020   \n",
       "7170  1.249367  1.048292     -0.823283 -0.479340 -0.490007 -0.162020   \n",
       "7186  1.470040  1.521619     -0.823283 -0.479340 -0.490007 -0.162020   \n",
       "7187  0.971667  0.972005     -0.823283 -0.479340 -0.490007 -0.162020   \n",
       "\n",
       "      galactose   glucose  lactose   maltose   sucrose       fat  \\\n",
       "1     -0.062457  0.372412 -0.07869 -0.150615  0.023213 -0.620268   \n",
       "2     -0.062457 -0.142311 -0.07869 -0.173220 -0.156322 -0.643289   \n",
       "3     -0.062457  0.520483 -0.07869 -0.173220 -0.156322 -0.747491   \n",
       "13    -0.062457 -0.142311 -0.07869 -0.173220 -0.156322 -0.709930   \n",
       "20    -0.062457 -0.142311 -0.07869 -0.173220 -0.156322 -0.740221   \n",
       "...         ...       ...      ...       ...       ...       ...   \n",
       "7166  -0.062457 -0.142311 -0.07869 -0.173220 -0.156322 -0.216185   \n",
       "7167  -0.062457 -0.142311 -0.07869 -0.173220 -0.156322  0.607731   \n",
       "7170  -0.062457 -0.142311 -0.07869 -0.173220 -0.156322  0.468392   \n",
       "7186  -0.062457 -0.142311 -0.07869 -0.173220 -0.156322 -0.436098   \n",
       "7187  -0.062457 -0.142311 -0.07869 -0.173220 -0.156322  0.962137   \n",
       "\n",
       "      saturated_fatty_acids  monounsaturated_fatty_acids  \\\n",
       "1                 -0.544198                    -0.549419   \n",
       "2                 -0.440772                    -0.557515   \n",
       "3                 -0.590602                    -0.625578   \n",
       "13                -0.577219                    -0.598133   \n",
       "20                -0.601075                    -0.617482   \n",
       "...                     ...                          ...   \n",
       "7166              -0.108383                    -0.134591   \n",
       "7167               0.730226                     0.691223   \n",
       "7170               0.652693                     0.670502   \n",
       "7186              -0.343601                    -0.272363   \n",
       "7187               1.073525                     1.032911   \n",
       "\n",
       "      polyunsaturated_fatty_acids  fatty_acids_total_trans   alcohol  \\\n",
       "1                       -0.273112                -0.366113 -0.024432   \n",
       "2                       -0.454119                -0.358304 -0.024432   \n",
       "3                       -0.463069                -0.366113 -0.024432   \n",
       "13                      -0.436950                -0.366113 -0.024432   \n",
       "20                      -0.426356                -0.366113 -0.024432   \n",
       "...                           ...                      ...       ...   \n",
       "7166                    -0.406265                 0.321081 -0.024432   \n",
       "7167                    -0.176489                 0.570970 -0.024432   \n",
       "7170                    -0.322793                 0.305463 -0.024432   \n",
       "7186                    -0.398593                 0.125856 -0.024432   \n",
       "7187                    -0.278774                 0.274227 -0.024432   \n",
       "\n",
       "           ash  caffeine  theobromine  water_mcar  \n",
       "1     0.265352 -0.032626    -0.083373   -0.003348  \n",
       "2    -0.642443 -0.032626    -0.083373    1.408102  \n",
       "3    -0.476551 -0.032626    -0.083373    1.651227  \n",
       "13    0.094852 -0.032626    -0.083373   -1.192960  \n",
       "20   -0.227714 -0.032626    -0.083373    1.330437  \n",
       "...        ...       ...          ...         ...  \n",
       "7166 -0.319876 -0.032626    -0.083373   -0.744752  \n",
       "7167 -0.310660 -0.032626    -0.083373    0.690429  \n",
       "7170 -0.393606 -0.032626    -0.083373    1.537408  \n",
       "7186 -0.200066 -0.032626    -0.083373    0.186553  \n",
       "7187 -0.352133 -0.032626    -0.083373    0.203349  \n",
       "\n",
       "[1476 rows x 72 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water_mcar_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "62e72f92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "water_mcar_pred = pd.DataFrame(auto_encoder.predict(water_mcar_test_df), columns = water_mcar_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cd9028b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chike\\Anaconda3\\envs\\imputation_research\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- water\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- water_mcar\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "water_mcar_true = pd.DataFrame(scaler_water_mcar.transform(water_df[water_df.water_mcar.isnull()].drop(['name','water_mcar', 'water_mar'], axis = 'columns')),columns = water_df.drop(['name','water_mcar', 'water_mar'], axis = 'columns').columns).drop('serving_size',axis = 'columns')# a = np.array(x) # your x\n",
    "# b = np.array(y) # your y\n",
    "# mses = ((a-b)**2).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1ca26b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>selenium</td>\n",
       "      <td>1.593192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>galactose</td>\n",
       "      <td>0.668080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>glucose</td>\n",
       "      <td>0.558205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>phosphorous</td>\n",
       "      <td>0.404909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>folic_acid</td>\n",
       "      <td>0.256179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>fructose</td>\n",
       "      <td>0.250337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>water</td>\n",
       "      <td>0.208576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>folate</td>\n",
       "      <td>0.201098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>protein</td>\n",
       "      <td>0.189536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>vitamin_b12</td>\n",
       "      <td>0.161144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        columns       mse\n",
       "31     selenium  1.593192\n",
       "57    galactose  0.668080\n",
       "58      glucose  0.558205\n",
       "29  phosphorous  0.404909\n",
       "6    folic_acid  0.256179\n",
       "56     fructose  0.250337\n",
       "71        water  0.208576\n",
       "5        folate  0.201098\n",
       "33      protein  0.189536\n",
       "17  vitamin_b12  0.161144"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "pd.DataFrame({'columns':water_mcar_true.columns,\n",
    "              'mse':list(mean_squared_error(water_mcar_true, water_mcar_pred, multioutput='raw_values'))}).nlargest(10,'mse')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imputation_research",
   "language": "python",
   "name": "imputation_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
